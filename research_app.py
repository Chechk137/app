import streamlit as st
import requests
import datetime
import random
import time
import json
import os
import math
from collections import Counter

# --- 1. ì„¤ì • ë° ìƒìˆ˜ ---

MISSIONS = [
    {"id": 1, "text": "Top Tier ì €ë„(Nature, Science ë“±) ë…¼ë¬¸ 1í¸ ìˆ˜ì§‘", "type": "journal", "target": "top_tier", "count": 1, "reward": 150},
    {"id": 2, "text": "5ì¸ ì´ìƒ í˜‘ì—… ì—°êµ¬(Team Science) ìˆ˜ì§‘", "type": "team", "target": 5, "count": 1, "reward": 100},
    {"id": 3, "text": "í•¨ì • ë…¼ë¬¸(ì°¸ê³ ë¬¸í—Œ ë¶€ì¡± ë“±) í”¼í•˜ê¸°", "type": "avoid_trap", "target": "trap", "count": 0, "reward": 0},
    {"id": 4, "text": "ì—°êµ¬ ì ìˆ˜ 1500ì  ë‹¬ì„±í•˜ê¸°", "type": "score", "target": 1500, "count": 1500, "reward": 500},
]

DATA_DIR = "user_data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# --- 2. ë°ì´í„° ê´€ë¦¬ í•¨ìˆ˜ (ì €ì¥/ë¡œë“œ) ---

def load_user_data(user_id):
    file_path = os.path.join(DATA_DIR, f"{user_id}.json")
    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                return {
                    "score": data.get("score", 0),
                    "inventory": data.get("inventory", []),
                    "mission_id": data.get("mission_id", 1),
                    "trash": data.get("trash", [])
                }
        except Exception as e:
            st.error(f"ë°ì´í„° ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
    return {"score": 0, "inventory": [], "mission_id": 1, "trash": []}

def save_user_data(user_id):
    file_path = os.path.join(DATA_DIR, f"{user_id}.json")
    data = {
        "score": st.session_state.score,
        "inventory": st.session_state.inventory,
        "mission_id": st.session_state.mission_id,
        "trash": st.session_state.trash
    }
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"ë°ì´í„° ì €ì¥ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")

# --- 3. í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

def get_current_year():
    return datetime.datetime.now().year

def evaluate_paper(paper_data):
    """
    ë…¼ë¬¸ì˜ ê°€ì¹˜ë¥¼ Raw Score(ì¸ê¸°ë„)ì™€ Debiased Score(ë‚´ì‹¤)ë¡œ ë¶„ë¦¬í•˜ì—¬ í‰ê°€
    """
    current_year = get_current_year()
    year = paper_data.get('year', current_year - 5)
    age = current_year - year
    title_lower = paper_data['title'].lower()
    citation_count = paper_data.get('citations', 0)
    
    # 1. í‚¤ì›Œë“œ (Evidence)
    evidence_keywords = [
        'in vivo', 'in vitro', 'randomized', 'efficacy', 'mechanism', 'signaling', 
        'experiment', 'analysis', 'clinical', 'activity', 'synthesis', 'design', 
        'evaluation', 'characterization', 'properties', 'performance', 'application'
    ]
    has_evidence = any(k in title_lower for k in evidence_keywords)
    
    # 2. ì €ë„ ê¶Œìœ„ (Journal Prestige)
    top_journals = ['nature', 'science', 'cell', 'lancet', 'nejm', 'jama', 'ieee', 'pnas', 'advanced materials', 'cancer discovery', 'chem', 'acs', 'angewandte']
    journal_lower = paper_data.get('journal', "").lower()
    is_top_tier = any(j in journal_lower for j in top_journals)

    # 3. ì—°êµ¬íŒ€ ê·œëª¨ (Team Size)
    author_count = paper_data.get('author_count', 1)
    is_big_team = author_count >= 5

    # 4. ë°ì´í„° ì‹ ë¢°ë„ (Reliability)
    ref_count = paper_data.get('ref_count') 
    integrity_status = "valid"
    risk_reason = ""

    if ref_count is None:
        if citation_count < 5 and not is_top_tier:
            integrity_status = "uncertain"
            risk_reason = "ë©”íƒ€ë°ì´í„° ëˆ„ë½"
    elif ref_count < 5:
        if citation_count < 5 and not is_top_tier:
            integrity_status = "suspected"
            risk_reason = "ì°¸ê³ ë¬¸í—Œ ë°ì´í„° ë¶€ì¡±"

    # --- [New] ì ìˆ˜ ë¶„ë¦¬ ë¡œì§ ---

    # 1. Raw Score: ê²€ìƒ‰ ì‹œìŠ¤í…œì´ ì„ í˜¸í•˜ëŠ” ì ìˆ˜ (ì¸ìš©ìˆ˜, ì €ë„ ì¸ì§€ë„ ì¤‘ì‹¬)
    # ì¸ìš©ìˆ˜ê°€ ë§ì„ìˆ˜ë¡ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ì¦ê°€ (ìµœëŒ€ 100ì )
    raw_score = min(99, int(10 + (math.log(citation_count + 1) * 15)))
    if is_top_tier: raw_score = min(99, raw_score + 20)

    # 2. Debiased Score: ë¬¸í—ŒëŸ‰ íš¨ê³¼ë¥¼ ì œê±°í•œ ë³¸ì—°ì˜ ê°€ì¹˜ (ì¦ê±°, ìµœì‹ ì„±, í¬ì†Œì„±)
    debiased_base = 40
    if has_evidence: debiased_base += 30 # ì‹¤í—˜ì  ê·¼ê±°ê°€ í•µì‹¬
    if is_big_team: debiased_base += 10
    
    # ë¬¸í—ŒëŸ‰ í¸í–¥ ì œê±°: ì¸ìš©ìˆ˜ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì˜¤íˆë ¤ 'í¬ì†Œì„±' ê´€ì ì—ì„œ ê°ì 
    # (ì´ë¯¸ ë‹¤ ì•„ëŠ” ë‚´ìš©ì¼ í™•ë¥ ì´ ë†’ìŒ)
    volume_discount = min(30, int(math.log(citation_count + 1) * 5))
    
    # ìµœì‹  ì—°êµ¬ ë³´ì • (ìµœì‹ ì¼ìˆ˜ë¡ í˜ë„í‹° ì™„í™”)
    if age <= 2: volume_discount = int(volume_discount * 0.2)
    elif age <= 5: volume_discount = int(volume_discount * 0.5)

    debiased_score = debiased_base - volume_discount
    
    # í•¨ì •/ì •ë³´ë¶€ì¡± í˜ë„í‹°
    if integrity_status != "valid":
        debiased_score = 10
        risk_reason = risk_reason or "ë°ì´í„° ì‹ ë¢°ë„ ë‚®ìŒ"
    elif age > 10 and citation_count < 5:
        debiased_score = 5
        risk_reason = "ë„íƒœëœ ì—°êµ¬ (Old & Low Cited)"

    debiased_score = max(5, min(99, debiased_score))

    # 3. Bias Penalty & Type
    bias_penalty = raw_score - debiased_score
    
    potential_type = "normal"
    if debiased_score > 75 and bias_penalty < 0:
        potential_type = "amazing" # ì¸ê¸°ëŠ” ë‚®ì€ë° ì‹¤ì†ì€ ê½‰ ì°¸ (ì €í‰ê°€ ìš°ëŸ‰ì£¼)
    elif bias_penalty > 30:
        potential_type = "bubble" # ì¸ê¸°ëŠ” ë§ì€ë° ì‹¤ì†ì€ ë³´í†µ (ê³ í‰ê°€)
    elif integrity_status != "valid":
        potential_type = "bad"

    # ì‹œê°ì  ì •ë ¬ì„ ìœ„í•œ ì¢…í•© ì ìˆ˜ (Debiased ë¹„ì¤‘ì„ ë†’ì„)
    ai_score = debiased_score

    return {
        "raw_score": raw_score,
        "debiased_score": debiased_score,
        "bias_penalty": bias_penalty,
        "ai_score": ai_score,
        "potential_type": potential_type,
        "risk_reason": risk_reason,
        "has_evidence": has_evidence,
        "is_top_tier": is_top_tier,
        "is_big_team": is_big_team,
        "integrity_status": integrity_status
    }

def search_crossref_api(query):
    is_exact_mode = query.startswith('"') and query.endswith('"')
    clean_query = query.strip('"') if is_exact_mode else query
    
    try:
        # ëŒ€ëŸ‰ ìˆ˜ì§‘ (rows=1000, í†µê³„ìš©)
        url = f"https://api.crossref.org/works?query={clean_query}&rows=1000&sort=relevance"
        response = requests.get(url, timeout=20)
        data = response.json()
    except Exception as e:
        st.error(f"API ì—°ê²° ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
        return [], {}, False

    # [ìˆ˜ì •] ë°ì´í„° ìœ íš¨ì„± ê²€ì‚¬ ê°•í™” (NoneType ì˜¤ë¥˜ ë°©ì§€)
    if not data or not isinstance(data, dict):
        return [], {}, False
        
    message = data.get('message')
    if not message or not isinstance(message, dict):
        return [], {}, False
        
    items = message.get('items')
    if not items:
        return [], {}, False

    valid_papers = []
    current_year = get_current_year()

    # --- í¸í–¥ ìš”ì•½ í†µê³„ ê³„ì‚° ---
    total_results = message.get('total-results', 0)
    citations_list = []
    years_list = []

    for item in items:
        # í•„í„°ë§
        if not item.get('DOI'): continue
        if not item.get('title'): continue
        
        title_str = item['title'][0].lower()
        invalid_titles = ["announcement", "editorial", "issue info", "table of contents", "front matter", "back matter", "author index", "subject index", "correction", "erratum", "publisher's note", "conference info", "trial number", "trial registration", "clinicaltrials.gov", "identifier", "&na;", "unknown", "calendar", "masthead", "abstracts", "session", "meeting", "symposium", "workshop", "chinese journal", "test", "protocol", "data descriptor", "dataset"]
        if any(inv in title_str for inv in invalid_titles): continue
        
        # í†µê³„ìš© ë°ì´í„° ìˆ˜ì§‘
        cit = item.get('is-referenced-by-count', 0)
        citations_list.append(cit)
        
        y = None
        if item.get('published') and item['published'].get('date-parts'): y = item['published']['date-parts'][0][0]
        elif item.get('created') and item['created'].get('date-parts'): y = item['created']['date-parts'][0][0]
        if y: years_list.append(y)

        # ì €ì ì²´í¬
        if not item.get('author'): continue
        authors_raw = item['author']
        valid_authors = []
        for a in authors_raw:
            given = a.get('given', '').strip()
            family = a.get('family', '').strip()
            full = f"{given} {family}".strip()
            if full and "&na;" not in full.lower() and "anonymous" not in full.lower():
                valid_authors.append(full)
        if not valid_authors: continue

        # ë©”íƒ€ë°ì´í„°
        journal = item.get('container-title', ["Unknown Journal"])[0]
        ref_count = item.get('reference-count')
        pub_year = y if y else current_year - 5
        
        # í‰ê°€ ì‹¤í–‰
        paper_data_for_eval = {
            'title': item['title'][0], 'year': pub_year, 'citations': cit, 
            'journal': journal, 'author_count': len(valid_authors), 'ref_count': ref_count
        }
        eval_result = evaluate_paper(paper_data_for_eval)

        paper_obj = {
            'id': item['DOI'],
            'title': item['title'][0],
            'authors': valid_authors[:3], 
            'author_full_count': len(valid_authors),
            'journal': journal,
            'year': pub_year,
            'citations': cit,
            'ref_count': ref_count if ref_count is not None else 0,
            'url': f"https://doi.org/{item['DOI']}",
            **eval_result,
            'is_reviewed': False
        }
        valid_papers.append(paper_obj)
    
    # í†µê³„ ì²˜ë¦¬
    avg_citations = int(sum(citations_list) / len(citations_list)) if citations_list else 0
    if years_list:
        year_counts = Counter(years_list)
        most_common_year = year_counts.most_common(1)[0][0]
        # ì§‘ì¤‘ ì‹œê¸° (ëŒ€ëµì )
        min_y, max_y = min(years_list), max(years_list)
        if max_y - min_y > 10:
             period_str = f"{most_common_year-2}~{most_common_year+2}"
        else:
             period_str = f"{min_y}~{max_y}"
    else:
        period_str = "Unknown"

    bias_summary = {
        "total_results": total_results,
        "avg_citations": avg_citations,
        "period": period_str,
        "is_high_exposure": total_results > 5000 or avg_citations > 100
    }

    # ì •ë ¬: ì¼ë°˜ ê²€ìƒ‰ì´ë©´ Debiased Score(ë‚´ì‹¤) ìˆœ, ë”°ì˜´í‘œë©´ ì •í™•ë„(API) ìˆœ
    if not is_exact_mode:
        valid_papers.sort(key=lambda x: x['debiased_score'], reverse=True)
            
    return valid_papers, bias_summary, is_exact_mode

# --- 3. Streamlit UI ---

st.set_page_config(page_title="Research Simulator", page_icon="ğŸ“", layout="wide")

if 'user_id' not in st.session_state: st.session_state['user_id'] = None
if 'score' not in st.session_state: st.session_state['score'] = 0
if 'inventory' not in st.session_state: st.session_state['inventory'] = []
if 'trash' not in st.session_state: st.session_state['trash'] = []
if 'mission_id' not in st.session_state: st.session_state['mission_id'] = 1
if 'search_results' not in st.session_state: st.session_state['search_results'] = []
if 'bias_summary' not in st.session_state: st.session_state['bias_summary'] = {}
if 'search_page' not in st.session_state: st.session_state['search_page'] = 1
if 'is_exact_search' not in st.session_state: st.session_state['is_exact_search'] = False

def get_level_info(score):
    level_threshold = 500
    level = (score // level_threshold) + 1
    progress = (score % level_threshold) / level_threshold
    next_milestone = (level) * level_threshold
    return level, progress, next_milestone

def check_mission(paper, action):
    current_m = next((m for m in MISSIONS if m['id'] == st.session_state.mission_id), None)
    if not current_m: return
    completed = False
    m_type = current_m['type']
    if m_type == "journal" and action == "collect" and paper['is_top_tier']: completed = True
    elif m_type == "team" and action == "collect" and paper['is_big_team']: completed = True
    elif m_type == "score" and st.session_state.score >= current_m['target']: completed = True
    if completed:
        st.session_state.score += current_m['reward']
        st.session_state.mission_id += 1
        st.toast(f"ğŸ‰ ë¯¸ì…˜ ì™„ë£Œ! ë³´ìƒ +{current_m['reward']}ì ", icon="ğŸ")
        if st.session_state.get("user_id"): save_user_data(st.session_state.user_id)

# ë¡œê·¸ì¸ í™”ë©´
if not st.session_state.get("user_id"):
    st.title("ğŸ“ AI ê¸°ë°˜ ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œ")
    st.caption("ìº¡ìŠ¤í†¤ ë””ìì¸ _ AI:D")
    st.markdown("---")
    st.markdown("### ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤!")
    st.info("ì—°êµ¬ì IDë¥¼ ì…ë ¥í•˜ì—¬ ê²€ìƒ‰ì„ ì‹œì‘í•˜ì„¸ìš”.")
    col1, col2 = st.columns([3, 1])
    with col1: user_input = st.text_input("ì—°êµ¬ì ì´ë¦„ (ID)", placeholder="ì˜ˆ: Dr.Kim")
    with col2:
        st.write(""); st.write("")
        if st.button("ë¡œê·¸ì¸ / ì‹œì‘", type="primary", use_container_width=True):
            if user_input:
                st.session_state.user_id = user_input
                saved_data = load_user_data(user_input)
                st.session_state.score = saved_data["score"]
                st.session_state.inventory = saved_data["inventory"]
                st.session_state.mission_id = saved_data["mission_id"]
                st.session_state.trash = saved_data["trash"]
                st.rerun()
            else: st.warning("ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop() 

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.title("ğŸ“ AI ê¸°ë°˜ ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œ")
    st.caption("ìº¡ìŠ¤í†¤ ë””ìì¸_AI:D")
    st.info(f"ğŸ‘¤ {st.session_state.user_id} ì—°êµ¬ì›")
    if st.button("ë¡œê·¸ì•„ì›ƒ (ì €ì¥ë¨)", use_container_width=True):
        save_user_data(st.session_state.user_id)
        st.session_state.user_id = None
        st.rerun()
    st.divider()
    current_level, progress, next_score = get_level_info(st.session_state.score)
    st.metric("ì—°êµ¬ ë ˆë²¨", f"Lv. {current_level}")
    st.write(f"í˜„ì¬ ì ìˆ˜: {st.session_state.score} / {next_score}")
    st.progress(progress)
    st.metric("ë³´ìœ  ë…¼ë¬¸", f"{len(st.session_state.inventory)}í¸")
    st.divider()
    
    st.markdown("#### ğŸ” í‰ê°€ ì§€í‘œ ê°€ì´ë“œ")
    st.markdown("""
    **1. Raw Score (ì¸ê¸°ë„)**
    : ê¸°ì¡´ ê²€ìƒ‰ ì—”ì§„ ì ìˆ˜. ì¸ìš©ìˆ˜ì™€ ì €ë„ ì¸ì§€ë„ì— ë¹„ë¡€.
    
    **2. Debiased Score (ë‚´ì‹¤)**
    : ë¬¸í—ŒëŸ‰ ê±°í’ˆì„ ëº€ ì§„ì§œ ê°€ì¹˜. ì¦ê±°ì™€ í¬ì†Œì„± ì¤‘ì‹¬.
    
    **3. Bias Penalty (í¸í–¥)**
    : ì¸ê¸°ë„ì™€ ë‚´ì‹¤ì˜ ì°¨ì´. ì–‘ìˆ˜ë©´ ê³¼ì—´(Bubble), ìŒìˆ˜ë©´ ì €í‰ê°€(Hidden Gem).
    """)
    st.markdown("#### ğŸ” Raw score ì§€í‘œ")
    st.markdown("""
    1. ì¦ê±° ì í•©ì„± ì§€í‘œ (Evidence Index)
       : ì œëª©ì— ì‹¤í—˜ì  ê²€ì¦(in vivo, clinical ë“±)ì„ ì•”ì‹œí•˜ëŠ” êµ¬ì²´ì ì¸ ë‹¨ì–´ í¬í•¨
    2. ì €ë„ ê¶Œìœ„ ì§€í‘œ (Prestige Index)
       : Nature, Science ë“± í•™ê³„ì—ì„œ ì¸ì •ë°›ëŠ” ìµœìƒìœ„ ì €ë„
    3. ì—°êµ¬ ê·œëª¨ ì§€í‘œ (Collaboration Index)
       : ì°¸ì—¬ ì €ì ìˆ˜ ë‹¤ìˆ˜(5ì¸ ì´ìƒ)ê°€ ì°¸ì—¬í•œ ì—°êµ¬ ìš°ëŒ€
    4. ë°ì´í„° ì‹ ë¢°ë„ ì§€í‘œ (Reliability Index)
       : ì°¸ê³  ë¬¸í—Œ ìˆ˜ë¥¼ í™•ì¸í•˜ì—¬ ì—°êµ¬ì˜ ê¹Šì´ë¥¼ 1ì°¨ì ìœ¼ë¡œ ê±°ë¦…ë‹ˆë‹¤. ì°¸ê³  ë¬¸í—Œì´ ë„ˆë¬´ ì ìœ¼ë©´ ì •ì‹ ë…¼ë¬¸ì´ ì•„ë‹Œ ì´ˆë¡ì´ë‚˜ ë‹¨ìˆœ íˆ¬ê³ ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì•„ ë°°ì œí•©ë‹ˆë‹¤.
    5. ì‹œì˜ì„± ëŒ€ë¹„ ì¸ìš© ì§€í‘œ (Opportunity Index)
       : ë°œí–‰ ì‹œì ê³¼ ì¸ìš© ìˆ˜ì˜ ìƒê´€ê´€ê³„ë¥¼ ë¶„ì„í•˜ì—¬ ìˆ¨ê²¨ì§„ ê°€ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤. ìµœì‹ ì´ë©´ì„œ ì¸ìš©ì´ ì ì€ ì—°êµ¬ëŠ” ê¸°íšŒ(Opportunity)ë¡œ, ì˜¤ë˜ë˜ì—ˆëŠ”ë° ì¸ìš©ì´ ì—†ëŠ” ì—°êµ¬ëŠ” í•¨ì •(Trap)ìœ¼ë¡œ ë¶„ë¥˜í•©ë‹ˆë‹¤.
    """)
    
    st.markdown("#### ğŸ“Š ê²€ìƒ‰ ë°©ë²•")
    st.markdown("""
    1. ì¼ë°˜ ê²€ìƒ‰
       : AI ì¶”ì²œ ì§€ìˆ˜ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì¶”ì²œ
    2. "í‚¤ì›Œë“œ"
       : ë”°ì˜´í‘œ ê²€ìƒ‰ì„ í†µí•´ ì •í™•ë„ ìˆœìœ¼ë¡œ ê²€ìƒ‰
    """)

tab_search, tab_inventory, tab_trash = st.tabs(["ğŸ” ë…¼ë¬¸ ê²€ìƒ‰", "ğŸ“š ë‚´ ì„œì¬", "ğŸ—‘ï¸ íœ´ì§€í†µ"])

with tab_search:
    col1, col2 = st.columns([4, 1])
    with col1: query = st.text_input("í‚¤ì›Œë“œ ì…ë ¥", placeholder='ì˜ˆ: "Immunotherapy" (ë”°ì˜´í‘œëŠ” ì •í™•ë„ìˆœ)')
    with col2:
        st.write(""); st.write("")
        search_btn = st.button("ê²€ìƒ‰", type="primary", use_container_width=True)

    if search_btn and query:
        with st.spinner("ë¬¸í—ŒëŸ‰ í¸í–¥ ë¶„ì„ ë° ë°ì´í„° ì²˜ë¦¬ ì¤‘..."):
            results, summary, is_exact = search_crossref_api(query)
            st.session_state.search_results = results
            st.session_state.bias_summary = summary
            st.session_state.is_exact_search = is_exact
            st.session_state.search_page = 1 
            if not results: st.error("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    if st.session_state.search_results:
        summary = st.session_state.bias_summary
        
        # [New] í¸í–¥ ìš”ì•½ ë°•ìŠ¤
        with st.container(border=True):
            st.markdown("### ğŸ” Search Bias Summary")
            bc1, bc2, bc3 = st.columns(3)
            with bc1: st.metric("PubMed ë…¼ë¬¸ ìˆ˜ (ì¶”ì •)", f"{summary['total_results']:,}í¸")
            with bc2: st.metric("í‰ê·  ì¸ìš©ìˆ˜ (Top 200)", f"{summary['avg_citations']:,}íšŒ")
            with bc3: st.metric("ì—°êµ¬ ì§‘ì¤‘ ì‹œê¸°", summary['period'])
            
            if summary['is_high_exposure']:
                st.warning("âš  **High Exposure Topic**: ì´ ì£¼ì œëŠ” ì—°êµ¬ê°€ ë§¤ìš° í™œë°œí•˜ì—¬, ìƒìœ„ ë…¸ì¶œ ë…¼ë¬¸ì´ ê³¼ëŒ€í‰ê°€(Bias)ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤. Debiased Scoreë¥¼ ì°¸ê³ í•˜ì—¬ ë‚´ì‹¤ ìˆëŠ” ì—°êµ¬ë¥¼ ì„ ë³„í•˜ì„¸ìš”.")
            else:
                st.success("âœ… **Niche Topic**: ë¹„êµì  ì—°êµ¬ê°€ ëœ ëœ ë¶„ì•¼ì…ë‹ˆë‹¤. ìˆ¨ê²¨ì§„ ëª…ì‘ì´ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

        st.divider()

        # í˜ì´ì§€ë„¤ì´ì…˜
        items_per_page = 50
        total_items = len(st.session_state.search_results)
        total_pages = max(1, math.ceil(total_items / items_per_page))
        current_page = st.session_state.search_page
        start_idx = (current_page - 1) * items_per_page
        end_idx = start_idx + items_per_page
        page_items = st.session_state.search_results[start_idx:end_idx]

        sort_mode = "ì •í™•ë„(Relevance)" if st.session_state.is_exact_search else "ë‚´ì‹¤(Debiased)"
        st.caption(f"ê²€ìƒ‰ ê²°ê³¼ ì´ {total_items}ê±´ ({sort_mode} ì •ë ¬) | í˜ì´ì§€: {current_page}/{total_pages}")
        
        for i, paper in enumerate(page_items):
            unique_key_idx = start_idx + i
            with st.container(border=True):
                c1, c2 = st.columns([5, 2])
                with c1:
                    st.markdown(f"#### {paper['title']}")
                    
                    # íƒœê·¸ í‘œì‹œ
                    tags = []
                    if paper['is_top_tier']: tags.append("ğŸ‘‘ Top Tier")
                    if paper['has_evidence']: tags.append("ğŸ”¬ Evidence")
                    if paper['is_big_team']: tags.append("ğŸ‘¥ Big Team")
                    if paper['integrity_status'] != "valid": tags.append("âš ï¸ ë°ì´í„° ë¶€ì¡±")
                    if paper['potential_type'] == "amazing": tags.append("ğŸ’ Hidden Gem")
                    
                    st.write(" ".join([f"`{t}`" for t in tags]))
                    
                    auth_display = ", ".join(paper['authors'])
                    if paper['author_full_count'] > 3: auth_display += f" ì™¸ {paper['author_full_count'] - 3}ëª…"
                    st.caption(f"{paper['year']} | {paper['journal']} | ì¸ìš© {paper['citations']}íšŒ | ì €ì: {auth_display}")
                    st.markdown(f"[ğŸ“„ ì›ë¬¸ ë³´ê¸°]({paper['url']})")

                with c2:
                    # [New] ì ìˆ˜ ë¹„êµ ëŒ€ì‹œë³´ë“œ
                    col_raw, col_deb = st.columns(2)
                    with col_raw:
                        st.metric("Raw Score", f"{paper['raw_score']}", help="ê²€ìƒ‰ ì—”ì§„ì´ ì„ í˜¸í•˜ëŠ” ì¸ê¸°ë„ ì ìˆ˜")
                    with col_deb:
                        st.metric("Debiased", f"{paper['debiased_score']}", delta=f"{-paper['bias_penalty']}", help="ë¬¸í—ŒëŸ‰ ê±°í’ˆì„ ëº€ ì§„ì§œ ë‚´ì‹¤ ì ìˆ˜")
                    
                    if paper['bias_penalty'] > 20:
                        st.caption("âš  High exposure (ê±°í’ˆ ì£¼ì˜)")
                    
                    # ìˆ˜ì§‘ ë²„íŠ¼
                    is_owned = any(p['id'] == paper['id'] for p in st.session_state.inventory)
                    if is_owned:
                        st.button("ë³´ìœ ì¤‘", key=f"owned_{unique_key_idx}", disabled=True, use_container_width=True)
                    else:
                        if st.button("ìˆ˜ì§‘í•˜ê¸°", key=f"collect_{unique_key_idx}", type="secondary", use_container_width=True):
                            st.session_state.inventory.append(paper)
                            st.session_state.score += paper['debiased_score'] # íšë“ ì‹œ Debiased ì ìˆ˜ ë¶€ì—¬
                            check_mission(paper, "collect")
                            save_user_data(st.session_state.user_id) 
                            st.rerun()
        
        st.divider()
        # í˜ì´ì§€ë„¤ì´ì…˜ ì»¨íŠ¸ë¡¤ëŸ¬ (ì¤‘ì•™)
        _, nav_col, _ = st.columns([1, 5, 1])
        with nav_col:
            pg_cols = st.columns([1, 1, 1, 1, 1, 1, 1, 0.5, 2.5], gap="small")
            with pg_cols[0]:
                if st.button("â—€", key="nav_prev", disabled=current_page==1, use_container_width=True):
                    st.session_state.search_page -= 1
                    st.rerun()
            
            # í˜ì´ì§€ ë²ˆí˜¸ ê³„ì‚°
            if total_pages <= 5: display_pages = range(1, total_pages + 1)
            else:
                if current_page <= 3: display_pages = range(1, 6)
                elif current_page >= total_pages - 2: display_pages = range(total_pages - 4, total_pages + 1)
                else: display_pages = range(current_page - 2, current_page + 3)

            for idx, p_num in enumerate(display_pages):
                if idx < 5:
                    with pg_cols[idx + 1]:
                        b_type = "primary" if p_num == current_page else "secondary"
                        if st.button(f"{p_num}", key=f"nav_p_{p_num}", type=b_type, use_container_width=True):
                            st.session_state.search_page = p_num
                            st.rerun()
            
            with pg_cols[6]:
                if st.button("â–¶", key="nav_next", disabled=current_page==total_pages, use_container_width=True):
                    st.session_state.search_page += 1
                    st.rerun()
            with pg_cols[8]:
                 new_page = st.number_input("ì´ë™", min_value=1, max_value=total_pages, value=current_page, label_visibility="collapsed", key="nav_input")
                 if new_page != current_page:
                    st.session_state.search_page = new_page
                    st.rerun()

with tab_inventory:
    if not st.session_state.inventory: st.info("ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
    
    cols = st.columns(2)
    for i, paper in enumerate(st.session_state.inventory):
        with cols[i % 2]:
            with st.container(border=True):
                status_emoji = "â“"
                status_text = "ë¯¸ê²€ì¦"
                if paper['is_reviewed']:
                    if paper['potential_type'] == "amazing": status_emoji, status_text = "âœ¨", "ëŒ€ì„±ê³µ"
                    elif paper['potential_type'] == "bad": status_emoji, status_text = "ğŸ’€", "ì‹¤íŒ¨"
                    elif paper['potential_type'] == "verified_user": status_emoji, status_text = "ğŸ›¡ï¸", "ì‚¬ìš©ì ìŠ¹ì¸"
                    else: status_emoji, status_text = "âœ…", "ê²€ì¦ë¨"

                st.markdown(f"**{paper['title']}**")
                st.caption(f"{status_emoji} {status_text} | {paper['journal']}")
                
                c_btn1, c_btn2 = st.columns([2, 1])
                with c_btn1:
                    if not paper['is_reviewed']:
                        if paper['integrity_status'] == "valid":
                            if st.button("ğŸ”¬ ì‹¬ì¸µ ê²€ì¦", key=f"rev_{i}", type="primary", use_container_width=True):
                                st.session_state.inventory[i]['is_reviewed'] = True
                                bonus = int(paper['debiased_score'] * 0.5) # ê²€ì¦ ì‹œ ì¶”ê°€ ë³´ë„ˆìŠ¤
                                st.session_state.score += bonus
                                st.session_state.inventory[i]['final_score'] = paper['debiased_score'] + bonus
                                
                                if paper['potential_type'] == 'amazing':
                                    st.toast(f"ëŒ€ë°•! ìˆ¨ê²¨ì§„ ëª…ì‘ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤! (+{bonus})", icon="ğŸ‰")
                                else:
                                    st.toast(f"ê²€ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (+{bonus})", icon="âœ…")
                                save_user_data(st.session_state.user_id) 
                                st.rerun()
                        else:
                            st.warning(paper['risk_reason'])
                            if st.button("ê°•ì œ ìŠ¹ì¸", key=f"force_{i}", use_container_width=True):
                                st.session_state.inventory[i]['is_reviewed'] = True
                                bonus = 10 # ê°•ì œ ìŠ¹ì¸ì€ ì†ŒëŸ‰ ë³´ë„ˆìŠ¤
                                st.session_state.score += bonus
                                st.session_state.inventory[i]['final_score'] = paper['debiased_score'] + bonus
                                st.session_state.inventory[i]['potential_type'] = "verified_user"
                                st.session_state.inventory[i]['reason'] = "ì‚¬ìš©ì ì§ì ‘ í™•ì¸ìœ¼ë¡œ ê²€ì¦ë¨"
                                save_user_data(st.session_state.user_id) 
                                st.rerun()
                    else:
                        st.success(f"ê°€ì¹˜: {paper.get('final_score', 0)}ì ")

                with c_btn2:
                    if st.button("ì‚­ì œ", key=f"del_{i}", use_container_width=True):
                        deduction = paper.get('final_score', paper['debiased_score'])
                        st.session_state.score = max(0, st.session_state.score - deduction)
                        removed = st.session_state.inventory.pop(i)
                        st.session_state.trash.append(removed)
                        st.toast(f"ë…¼ë¬¸ ì‚­ì œ. {deduction}ì  ì°¨ê°ë¨", icon="ğŸ—‘ï¸")
                        save_user_data(st.session_state.user_id) 
                        st.rerun()
                
                st.markdown(f"[ğŸ“„ ì›ë¬¸ ë³´ê¸°]({paper['url']})")

with tab_trash:
    if not st.session_state.trash: st.info("íœ´ì§€í†µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    
    if st.session_state.trash:
        if st.button("íœ´ì§€í†µ ë¹„ìš°ê¸° (ì „ì²´ ì‚­ì œ)", type="primary"):
            st.session_state.trash = []
            save_user_data(st.session_state.user_id)
            st.toast("íœ´ì§€í†µì„ ë¹„ì› ìŠµë‹ˆë‹¤.", icon="ğŸ§¹")
            st.rerun()

    cols = st.columns(2)
    for i, paper in enumerate(st.session_state.trash):
        with cols[i % 2]:
            with st.container(border=True):
                st.markdown(f"**{paper['title']}**")
                st.caption(f"ì‚­ì œë¨ | {paper['journal']}")
                
                col_res, col_del = st.columns(2)
                with col_res:
                    if st.button("ë³µêµ¬", key=f"restore_{i}", use_container_width=True):
                        restored = st.session_state.trash.pop(i)
                        st.session_state.inventory.append(restored)
                        restore_score = restored.get('final_score', restored['debiased_score'])
                        st.session_state.score += restore_score
                        st.toast(f"ë³µêµ¬ ì™„ë£Œ (+{restore_score}ì )", icon="â™»ï¸")
                        save_user_data(st.session_state.user_id)
                        st.rerun()
                with col_del:
                    if st.button("ì˜êµ¬ ì‚­ì œ", key=f"perm_del_{i}", use_container_width=True):
                        st.session_state.trash.pop(i)
                        st.toast("ì˜êµ¬ ì‚­ì œë¨", icon="ğŸ”¥")
                        save_user_data(st.session_state.user_id)
                        st.rerun()
