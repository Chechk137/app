import streamlit as st
import requests
import datetime
import random
import time
import json
import os
import math
import re
import pandas as pd
import altair as alt
from collections import Counter

# ==============================================================================
# [SECTION 1] ì„¤ì • ë° ìƒìˆ˜ ì •ì˜
# : ì•± ì „ë°˜ì—ì„œ ì‚¬ìš©ë˜ëŠ” ê³ ì •ê°’ê³¼ í™˜ê²½ ì„¤ì •ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
# ==============================================================================

# ë…¼ë¬¸ í‰ê°€ ë° ì‹œê°ì  ê°•ì¡°(í•˜ì´ë¼ì´íŒ…)ì— ì‚¬ìš©ë˜ëŠ” í•µì‹¬ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
EVIDENCE_KEYWORDS = [
    'in vivo', 'in vitro', 'randomized', 'efficacy', 'mechanism', 'signaling', 
    'experiment', 'analysis', 'clinical', 'activity', 'synthesis', 'design', 
    'evaluation', 'characterization', 'properties', 'performance', 'application'
]

# ë°ì´í„° ì €ì¥ ê²½ë¡œ ì„¤ì •
DATA_DIR = "user_data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)


# ==============================================================================
# [SECTION 2] ë°ì´í„° ê´€ë¦¬ (Persistence Layer)
# : ì‚¬ìš©ì ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œ ë¡œë“œí•˜ê³  ì €ì¥í•˜ëŠ” í•¨ìˆ˜ë“¤ì…ë‹ˆë‹¤.
# ==============================================================================

def load_user_data(user_id):
    """ì‚¬ìš©ì IDì— í•´ë‹¹í•˜ëŠ” JSON íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤. ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ì„ ë°˜í™˜í•©ë‹ˆë‹¤."""
    file_path = os.path.join(DATA_DIR, f"{user_id}.json")
    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                return {
                    "score": data.get("score", 0),
                    "inventory": data.get("inventory", []),
                    "trash": data.get("trash", [])
                }
        except Exception as e:
            st.error(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
    return {"score": 0, "inventory": [], "trash": []}

def save_user_data(user_id):
    """í˜„ì¬ ì„¸ì…˜ ìƒíƒœ(ì ìˆ˜, ì¸ë²¤í† ë¦¬ ë“±)ë¥¼ JSON íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    file_path = os.path.join(DATA_DIR, f"{user_id}.json")
    data = {
        "score": st.session_state.score,
        "inventory": st.session_state.inventory,
        "trash": st.session_state.trash
    }
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")


# ==============================================================================
# [SECTION 3] ìœ í‹¸ë¦¬í‹° ë° í…ìŠ¤íŠ¸ ì²˜ë¦¬ í•¨ìˆ˜
# : ë‚ ì§œ ê³„ì‚°, ë²ˆì—­, í…ìŠ¤íŠ¸ í•˜ì´ë¼ì´íŒ… ë“± ë³´ì¡° ê¸°ëŠ¥ì„ ë‹´ë‹¹í•©ë‹ˆë‹¤.
# ==============================================================================

def get_current_year():
    return datetime.datetime.now().year

@st.cache_data
def get_translated_title(text):
    """êµ¬ê¸€ ë²ˆì—­ API(ë¹„ê³µì‹)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ë¬¸ ì œëª©ì„ í•œê¸€ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    try:
        url = "https://translate.googleapis.com/translate_a/single"
        params = {
            "client": "gtx", "sl": "auto", "tl": "ko", "dt": "t", "q": text
        }
        response = requests.get(url, params=params, timeout=2)
        if response.status_code == 200:
            return response.json()[0][0][0]
    except Exception:
        pass
    return "ë²ˆì—­ ì‹¤íŒ¨ (ì—°ê²° í™•ì¸ í•„ìš”)"

def highlight_text(text):
    """ì œëª© ë‚´ì˜ EVIDENCE_KEYWORDSë¥¼ ì°¾ì•„ HTML í˜•ê´‘íœ ìŠ¤íƒ€ì¼ì„ ì ìš©í•©ë‹ˆë‹¤."""
    pattern = re.compile('|'.join(map(re.escape, EVIDENCE_KEYWORDS)), re.IGNORECASE)
    def replace(match):
        return f"<span style='background-color: #d1fae5; color: #065f46; padding: 0 4px; border-radius: 4px; font-weight: bold;'>{match.group(0)}</span>"
    return pattern.sub(replace, text)


# ==============================================================================
# [SECTION 4] í•µì‹¬ í‰ê°€ ì•Œê³ ë¦¬ì¦˜ (Scoring Logic)
# : ë…¼ë¬¸ì˜ ê°€ì¹˜ë¥¼ ê³„ì‚°í•˜ëŠ” ê°€ì¥ ì¤‘ìš”í•œ ë¡œì§ì…ë‹ˆë‹¤.
# : [Update] PubMed ì£¼ì œ ê³¼ì—´ë„(Multiplier)ë¥¼ ì¸ì(topic_multiplier)ë¡œ ë°›ë„ë¡ ìˆ˜ì •ë˜ì—ˆìŠµë‹ˆë‹¤.
# ==============================================================================

def evaluate_paper(paper_data, topic_multiplier=1.0):
    """
    ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ì™€ ì£¼ì œ ê³¼ì—´ë„ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Impact(ì¸ê¸°ë„)ì™€ Potential(ë‚´ì‹¤)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    topic_multiplier: PubMed ë¬¸í—Œ ìˆ˜ì— ë”°ë¥¸ ê³¼ì—´ ê°€ì¤‘ì¹˜ (ê¸°ë³¸ 1.0 ~ ìµœëŒ€ 2.0)
    """
    current_year = get_current_year()
    year = paper_data.get('year', current_year - 5)
    age = current_year - year
    title_lower = paper_data['title'].lower()
    citation_count = paper_data.get('citations', 0)
    
    # ì§€í‘œ 1: ì¦ê±° í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
    has_evidence = any(k in title_lower for k in EVIDENCE_KEYWORDS)
    
    # ì§€í‘œ 2: ëŒ€ê·œëª¨ ì—°êµ¬íŒ€ ì—¬ë¶€ (5ì¸ ì´ìƒ)
    author_count = paper_data.get('author_count', 1)
    is_big_team = author_count >= 5

    # ì§€í‘œ 3: ë°ì´í„° ì‹ ë¢°ë„ (ì°¸ê³ ë¬¸í—Œ ìˆ˜ ê¸°ë°˜)
    ref_count = paper_data.get('ref_count') 
    integrity_status = "valid"
    risk_reason = ""

    if ref_count is None:
        if citation_count < 5:
            integrity_status = "uncertain"
            risk_reason = "ë©”íƒ€ë°ì´í„° ëˆ„ë½"
    elif ref_count < 5:
        if citation_count < 5:
            integrity_status = "suspected"
            risk_reason = "ì°¸ê³ ë¬¸í—Œ ë¶€ì¡±"

    score_breakdown = {
        "Base": 30, "Evidence": 0, "Team": 0, "Volume Penalty": 0, "Integrity Penalty": 0
    }

    # 1. Impact (Raw Score): ì¸ê¸°ë„ ê¸°ë°˜ ì ìˆ˜
    raw_score = min(99, int(5 + (math.log(citation_count + 1) * 15)))

    # 2. Potential (Debiased Score): ë‚´ì‹¤ ê¸°ë°˜ ì ìˆ˜
    debiased_base = 30
    if has_evidence: 
        debiased_base += 30 
        score_breakdown["Evidence"] = 30
    if is_big_team: 
        debiased_base += 10
        score_breakdown["Team"] = 10
    
    # [Logic Update] ë¬¸í—ŒëŸ‰ í¸í–¥ ì œê±° (Volume Discount)
    # PubMed ì£¼ì œ ê³¼ì—´ë„(Multiplier)ë¥¼ ê³±í•˜ì—¬, ê³¼ì—´ëœ ì£¼ì œì¼ìˆ˜ë¡ ì¸ìš©ìˆ˜ ê°ì ì„ í¬ê²Œ ì ìš©
    base_volume_discount = min(25, int(math.log(citation_count + 1) * 4))
    
    # ìµœì‹  ì—°êµ¬ ë³´ì • (ì˜¤ë˜ë ìˆ˜ë¡ í˜ë„í‹° ê·¸ëŒ€ë¡œ, ìµœì‹ ì¼ìˆ˜ë¡ í˜ë„í‹° ì™„í™”)
    if age <= 2: base_volume_discount = int(base_volume_discount * 0.1)
    elif age <= 5: base_volume_discount = int(base_volume_discount * 0.5)

    # ìµœì¢… ê°ì  = ì¸ìš© ê¸°ë°˜ ê°ì  * ì£¼ì œ ê³¼ì—´ë„
    final_volume_penalty = int(base_volume_discount * topic_multiplier)

    score_breakdown["Volume Penalty"] = -final_volume_penalty
    debiased_score = debiased_base - final_volume_penalty
    
    # ì‹ ë¢°ë„ íŒ¨ë„í‹° ì ìš©
    if integrity_status != "valid":
        penalty = debiased_score - 5
        debiased_score = 5
        score_breakdown["Integrity Penalty"] = -penalty
        risk_reason = risk_reason or "ë°ì´í„° ì‹ ë¢°ë„ ë‚®ìŒ"
    elif age > 10 and citation_count < 5:
        penalty = debiased_score - 5
        debiased_score = 5
        score_breakdown["Integrity Penalty"] = -penalty
        risk_reason = "ë„íƒœëœ ì—°êµ¬"

    debiased_score = max(5, min(95, debiased_score))

    # 3. Bias Penalty: ì¸ê¸°ë„ì™€ ë‚´ì‹¤ì˜ ê´´ë¦¬
    bias_penalty = raw_score - debiased_score
    
    # ë…¼ë¬¸ ìœ í˜• ë¶„ë¥˜
    potential_type = "normal"
    if debiased_score > 70 and bias_penalty < 0:
        potential_type = "amazing" 
    elif bias_penalty > 30:
        potential_type = "bubble" 
    elif integrity_status != "valid":
        potential_type = "bad"

    return {
        "raw_score": raw_score, # Impact
        "debiased_score": debiased_score, # Potential
        "bias_penalty": bias_penalty,
        "potential_type": potential_type,
        "risk_reason": risk_reason,
        "has_evidence": has_evidence,
        "is_big_team": is_big_team,
        "integrity_status": integrity_status,
        "score_breakdown": score_breakdown,
        "age": age,
        "citation_count": citation_count
    }


# ==============================================================================
# [SECTION 5] ì™¸ë¶€ API í†µì‹ 
# : Crossref ë° PubMed APIì™€ í†µì‹ í•˜ì—¬ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤.
# ==============================================================================

def get_pubmed_count(query):
    """PubMedì—ì„œ í•´ë‹¹ í‚¤ì›Œë“œì˜ ì „ì²´ ë¬¸í—Œ ìˆ˜ë¥¼ ì¡°íšŒí•©ë‹ˆë‹¤."""
    try:
        url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        params = {"db": "pubmed", "term": query, "retmode": "json", "rettype": "count"}
        response = requests.get(url, params=params, timeout=5)
        data = response.json()
        return int(data["esearchresult"]["count"])
    except Exception:
        return None

def search_crossref_api(query):
    """Crossref APIë¥¼ í†µí•´ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ ê²€ìƒ‰í•˜ê³  í‰ê°€í•©ë‹ˆë‹¤."""
    is_exact_mode = query.startswith('"') and query.endswith('"')
    clean_query = query.strip('"') if is_exact_mode else query
    
    try:
        # [Modified] ì œëª© ìš°ì„  ê²€ìƒ‰ (query.title)
        url = f"https://api.crossref.org/works?query.title={clean_query}&rows=1000&sort=relevance"
        response = requests.get(url, timeout=20)
        data = response.json()
    except Exception as e:
        st.error(f"API ì—°ê²° ì˜¤ë¥˜: {e}")
        return [], {}, False

    if not data or not isinstance(data, dict): return [], {}, False
    items = data.get('message', {}).get('items', [])
    if not items: return [], {}, False

    valid_papers = []
    current_year = get_current_year()
    pubmed_count = get_pubmed_count(clean_query)
    
    # [New] ì£¼ì œ ê³¼ì—´ë„(Multiplier) ì‚°ì •
    # ë¬¸í—ŒëŸ‰ì´ ë§ì„ìˆ˜ë¡ ê±°í’ˆì¼ í™•ë¥ ì´ ë†’ë‹¤ê³  ê°€ì •í•˜ì—¬ í˜ë„í‹°ë¥¼ ê°•í™”í•¨
    topic_multiplier = 1.0
    if pubmed_count:
        if pubmed_count > 10000: topic_multiplier = 2.0  # ë§¤ìš° ê³¼ì—´ë¨ -> ê°ì  2ë°°
        elif pubmed_count > 5000: topic_multiplier = 1.5 # ê³¼ì—´ë¨ -> ê°ì  1.5ë°°
        elif pubmed_count > 1000: topic_multiplier = 1.2 # ë³´í†µ -> ê°ì  1.2ë°°
        # ê·¸ ì™¸(1000 ì´í•˜)ëŠ” 1.0ë°° (ê¸°ë³¸)

    citations_list = []
    years_list = []

    # [New] ê²€ìƒ‰ì–´ ë‹¨ì–´ ê²½ê³„ íŒ¨í„´ (ì—„ê²©í•œ í•„í„°ë§ìš©)
    word_pattern = re.compile(r'\b' + re.escape(clean_query) + r'\b', re.IGNORECASE)

    for idx, item in enumerate(items):
        if not item.get('DOI') or not item.get('title'): continue
        
        raw_title = item['title'][0]
        title_str = raw_title.lower()

        # [Check] ì œëª© ë‚´ ë‹¨ì–´ ë‹¨ìœ„ í¬í•¨ ì—¬ë¶€ í™•ì¸
        if not word_pattern.search(raw_title):
            continue

        invalid_titles = ["announcement", "editorial", "issue info", "correction", "erratum", "author index", "front matter", "back matter"]
        if any(inv in title_str for inv in invalid_titles): continue
        
        cit = item.get('is-referenced-by-count', 0)
        citations_list.append(cit)
        
        # ì—°ë„ ì¶”ì¶œ
        y = None
        if item.get('published') and item['published'].get('date-parts'): y = item['published']['date-parts'][0][0]
        elif item.get('created') and item['created'].get('date-parts'): y = item['created']['date-parts'][0][0]
        if y: years_list.append(y)

        # ì €ì ì •ë³´ ì •ì œ
        if not item.get('author'): continue
        valid_authors = []
        for a in item['author']:
            given = a.get('given', '').strip()
            family = a.get('family', '').strip()
            full = f"{given} {family}".strip()
            if full and "anonymous" not in full.lower():
                valid_authors.append(full)
        if not valid_authors: continue

        pub_year = y if y else current_year - 5
        
        paper_data_for_eval = {
            'title': item['title'][0], 'year': pub_year, 'citations': cit, 
            'journal': item.get('container-title', ["Unknown"])[0], 
            'author_count': len(valid_authors), 
            'ref_count': item.get('reference-count')
        }
        
        # [Modified] topic_multiplierë¥¼ í‰ê°€ í•¨ìˆ˜ì— ì „ë‹¬
        eval_result = evaluate_paper(paper_data_for_eval, topic_multiplier)

        # ê²°ê³¼ ê°ì²´ ìƒì„±
        paper_obj = {
            'id': item['DOI'],
            'title': item['title'][0],
            'authors': valid_authors[:3], 
            'author_full_count': len(valid_authors),
            'journal': item.get('container-title', ["Unknown"])[0],
            'year': pub_year,
            'citations': cit,
            'ref_count': item.get('reference-count') if item.get('reference-count') else 0,
            'url': f"https://doi.org/{item['DOI']}",
            **eval_result,
            'is_reviewed': False,
            'original_rank': idx
        }
        valid_papers.append(paper_obj)
    
    # í†µê³„ ì •ë³´ ìƒì„±
    avg_citations = int(sum(citations_list) / len(citations_list)) if citations_list else 0
    period_str = "Unknown"
    if years_list:
        min_y, max_y = min(years_list), max(years_list)
        period_str = f"{min_y}~{max_y}"

    bias_summary = {
        "pubmed_count": pubmed_count if pubmed_count is not None else "ì§‘ê³„ ë¶ˆê°€",
        "avg_citations": avg_citations,
        "period": period_str,
        "is_high_exposure": (pubmed_count > 5000 if pubmed_count else False) or avg_citations > 100,
        "multiplier": topic_multiplier # UI í‘œì‹œìš©
    }

    # ê¸°ë³¸ ì •ë ¬: Potential(ë‚´ì‹¤) ìˆœ
    if not is_exact_mode:
        valid_papers.sort(key=lambda x: x['debiased_score'], reverse=True)
            
    return valid_papers, bias_summary, is_exact_mode


# ==============================================================================
# [SECTION 6] ë‚´ë³´ë‚´ê¸° ìœ í‹¸ë¦¬í‹° (Export)
# : BibTeX ë° CSV ë³€í™˜ í•¨ìˆ˜ì…ë‹ˆë‹¤.
# ==============================================================================

def convert_to_bibtex(inventory_list):
    bibtex_entries = []
    for paper in inventory_list:
        first_author = paper['authors'][0].split()[-1] if paper['authors'] else "Unknown"
        safe_key = re.sub(r'\W+', '', f"{first_author}{paper['year']}")
        authors_formatted = " and ".join(paper['authors'])
        
        entry = f"""@article{{{safe_key},
  title = {{{paper['title']}}},
  author = {{{authors_formatted}}},
  journal = {{{paper['journal']}}},
  year = {{{paper['year']}}},
  doi = {{{paper['id']}}}
}}"""
        bibtex_entries.append(entry)
    return "\n\n".join(bibtex_entries)

def convert_to_csv(inventory_list):
    lines = ["DOI,Title,Authors,Journal,Year,Citations,MyScore"]
    for paper in inventory_list:
        safe_title = paper['title'].replace('"', '""')
        safe_authors = "; ".join(paper['authors']).replace('"', '""')
        safe_journal = paper['journal'].replace('"', '""')
        score = paper.get('final_score', paper.get('debiased_score', 0))
        line = f"\"{paper['id']}\",\"{safe_title}\",\"{safe_authors}\",\"{safe_journal}\",{paper['year']},{paper['citations']},{score}"
        lines.append(line)
    return "\n".join(lines)


# ==============================================================================
# [SECTION 7] Streamlit UI êµ¬ì„± - ë©”ì¸ ë° ì‚¬ì´ë“œë°”
# ==============================================================================

st.set_page_config(page_title="Research Simulator", page_icon="ğŸ“", layout="wide")

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if 'user_id' not in st.session_state: st.session_state['user_id'] = None
if 'score' not in st.session_state: st.session_state['score'] = 0
if 'inventory' not in st.session_state: st.session_state['inventory'] = []
if 'trash' not in st.session_state: st.session_state['trash'] = []
if 'search_results' not in st.session_state: st.session_state['search_results'] = []
if 'bias_summary' not in st.session_state: st.session_state['bias_summary'] = {}
if 'search_page' not in st.session_state: st.session_state['search_page'] = 1
if 'analysis_page' not in st.session_state: st.session_state['analysis_page'] = 1
if 'is_exact_search' not in st.session_state: st.session_state['is_exact_search'] = False
if 'sort_option' not in st.session_state: st.session_state['sort_option'] = "Potential"
if 'analysis_weights' not in st.session_state: st.session_state['analysis_weights'] = {"evidence": 1.0, "recency": 1.0, "team": 1.0, "scarcity": 1.0}
if 'current_preset' not in st.session_state: st.session_state['current_preset'] = "âš–ï¸ ë°¸ëŸ°ìŠ¤"

def get_level_info(score):
    level_threshold = 500
    level = (score // level_threshold) + 1
    progress = (score % level_threshold) / level_threshold
    next_milestone = (level) * level_threshold
    return level, progress, next_milestone

# ------------------------------------------------------------------------------
# [UI Part 1] ë¡œê·¸ì¸ í™”ë©´
# ------------------------------------------------------------------------------
if not st.session_state.get("user_id"):
    st.title("ğŸ“ AI ê¸°ë°˜ ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œ")
    st.caption("ìº¡ìŠ¤í†¤ ë””ìì¸ _ AI:D")
    st.markdown("---")
    st.markdown("### ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤!")
    st.info("ì—°êµ¬ì IDë¥¼ ì…ë ¥í•˜ì—¬ ê²€ìƒ‰ì„ ì‹œì‘í•˜ì„¸ìš”.")
    col1, col2 = st.columns([3, 1])
    with col1: user_input = st.text_input("ì—°êµ¬ì ì´ë¦„ (ID)", placeholder="ì˜ˆ: Dr.Kim")
    with col2:
        st.write(""); st.write("")
        if st.button("ë¡œê·¸ì¸ / ì‹œì‘", type="primary", use_container_width=True):
            if user_input:
                st.session_state.user_id = user_input
                saved_data = load_user_data(user_input)
                st.session_state.score = saved_data["score"]
                st.session_state.inventory = saved_data["inventory"]
                st.session_state.trash = saved_data["trash"]
                st.rerun()
            else: st.warning("ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop() 

# ------------------------------------------------------------------------------
# [UI Part 2] ì‚¬ì´ë“œë°” (ì •ë³´ ë° ê°€ì´ë“œ)
# ------------------------------------------------------------------------------
with st.sidebar:
    st.title("ğŸ“ AI ê¸°ë°˜ ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œ")
    st.caption("ìº¡ìŠ¤í†¤ ë””ìì¸ _ AI:D")
    st.info(f"ğŸ‘¤ {st.session_state.user_id} ì—°êµ¬ì›")
    if st.button("ë¡œê·¸ì•„ì›ƒ (ì €ì¥ë¨)", use_container_width=True):
        save_user_data(st.session_state.user_id)
        st.session_state.user_id = None
        st.rerun()
    st.divider()
    
    # ë ˆë²¨ ì •ë³´
    current_level, progress, next_score = get_level_info(st.session_state.score)
    st.metric("ì—°êµ¬ ë ˆë²¨", f"Lv. {current_level}")
    st.write(f"í˜„ì¬ ì ìˆ˜: {st.session_state.score} / {next_score}")
    st.progress(progress)
    st.metric("ë³´ìœ  ë…¼ë¬¸", f"{len(st.session_state.inventory)}í¸")
    st.divider()
    
    # ê°€ì´ë“œ
    st.markdown("#### ğŸ” í‰ê°€ ì§€í‘œ ê°€ì´ë“œ")
    st.markdown("""
    **1. Impact (ì˜í–¥ë ¥)**
    : ê¸°ì¡´ì˜ ì¸ê¸°ë„ ì ìˆ˜(Raw Score). ì¸ìš©ìˆ˜ì™€ ì €ë„ ì¸ì§€ë„ ë“± í•™ê³„ì—ì„œì˜ í˜„ì¬ ìœ„ìƒì„ ë°˜ì˜í•©ë‹ˆë‹¤.
    
    **2. Potential (ì ì¬ë ¥)**
    : ì¸ìš© ê±°í’ˆì„ ì œê±°í•œ ë‚´ì‹¤ ì ìˆ˜(Debiased Score). ì¦ê±° ê¸°ë°˜ì˜ í¬ì†Œì„±ê³¼ ë¯¸ë˜ ê°€ì¹˜ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
    
    **3. Bias Penalty (í¸í–¥)**
    : Impactì™€ Potentialì˜ ê´´ë¦¬. ì–‘ìˆ˜ë©´ ê³¼ì—´(Bubble), ìŒìˆ˜ë©´ ì €í‰ê°€(Hidden Gem)ëœ ì—°êµ¬ì…ë‹ˆë‹¤.
    """)

    st.markdown("#### ğŸ“Š ì ìˆ˜ ìƒì„¸ ì§€í‘œ")
    st.markdown("""
    **1. Evidence (ì¦ê±°)**
    - **ë°©ì‹**: ì œëª© ë‚´ ì‹¤í—˜ í‚¤ì›Œë“œ(in vivo, clinical ë“±) í¬í•¨ ì—¬ë¶€
    - **ì ìˆ˜**: í¬í•¨ ì‹œ 30ì  (ë¯¸í¬í•¨ 0ì )
    - **ì˜ë¯¸**: ì‹¤ì§ˆì  ì‹¤í—˜ ë°ì´í„°ê°€ ìˆëŠ” ë…¼ë¬¸ ìš°ëŒ€

    **2. Recency (ìµœì‹ ì„±)**
    - **ë°©ì‹**: (5 - ê²½ê³¼ë…„ìˆ˜) * 10 (ìµœëŒ€ 50ì )
    - **ì ìˆ˜**: ìµœì‹ ìˆœ 50ì  ~ 5ë…„ ì´ìƒ 0ì 
    - **ì˜ë¯¸**: ìµœì‹  ì—°êµ¬ì¼ìˆ˜ë¡ ê³ ë“ì 

    **3. Scarcity (í¬ì†Œì„±)**
    - **ë°©ì‹**: 50 - ì¸ìš© íšŸìˆ˜ (ìµœì†Œ 0ì )
    - **ì ìˆ˜**: ì¸ìš© 0íšŒ ì‹œ 50ì  ~ 50íšŒ ì´ìƒ 0ì 
    - **ì˜ë¯¸**: ì¸ìš©ì´ ì ì€ ìˆ¨ê²¨ì§„ ë…¼ë¬¸(Hidden Gem) ë°œêµ´

    **4. Team (ê·œëª¨)**
    - **ë°©ì‹**: ì €ì 5ëª… ì´ìƒ ì—¬ë¶€
    - **ì ìˆ˜**: 5ëª… ì´ìƒ ì‹œ 10ì  (ë¯¸ë§Œ 0ì )
    - **ì˜ë¯¸**: ëŒ€ê·œëª¨ í˜‘ì—… ì—°êµ¬ ë°˜ì˜
    """)

    st.markdown("#### ğŸ“Š ê²€ìƒ‰ ë°©ë²•")
    st.markdown("""
    1. ì¼ë°˜ ê²€ìƒ‰
        : AI ì¶”ì²œ ì§€ìˆ˜(Potential)ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì¶”ì²œ
    2. "í‚¤ì›Œë“œ"
        : ë”°ì˜´í‘œ ê²€ìƒ‰ ì‹œ ì •í™•ë„ ìˆœìœ¼ë¡œ ê²°ê³¼ ë…¸ì¶œ
    """)
    
    st.divider()
    # ì˜µì…˜ ì„¤ì •
    show_translation = st.checkbox("í•œê¸€ ë²ˆì—­ í•­ìƒ ë³´ê¸° (ëª¨ë°”ì¼ìš©)", value=False)
    show_highlight = st.checkbox("í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŒ… (Visual Evidence)", value=True, help="ì ìˆ˜ì— ê¸ì •ì  ì˜í–¥ì„ ì¤€ í•µì‹¬ ë‹¨ì–´ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.")


# ==============================================================================
# [SECTION 8] ë©”ì¸ ê¸°ëŠ¥ íƒ­ êµ¬ì„±
# ==============================================================================
tab_search, tab_analysis, tab_inventory, tab_trash = st.tabs(["ğŸ” ë…¼ë¬¸ ê²€ìƒ‰", "ğŸ“Š ì§€í‘œ ë¶„ì„", "ğŸ“š ë‚´ ì„œì¬", "ğŸ—‘ï¸ íœ´ì§€í†µ"])

# ------------------------------------------------------------------------------
# [Tab 1] ë…¼ë¬¸ ê²€ìƒ‰
# ------------------------------------------------------------------------------
with tab_search:
    col1, col2 = st.columns([4, 1])
    with col1: query = st.text_input("í‚¤ì›Œë“œ ì…ë ¥", placeholder='ì˜ˆ: "Immunotherapy" (ë”°ì˜´í‘œëŠ” ì •í™•ë„ìˆœ)')
    with col2:
        st.write(""); st.write("")
        search_btn = st.button("ê²€ìƒ‰", type="primary", use_container_width=True)

    if search_btn and query:
        with st.spinner("ë¬¸í—ŒëŸ‰ í¸í–¥ ë¶„ì„ ë° ë°ì´í„° ì²˜ë¦¬ ì¤‘..."):
            results, summary, is_exact = search_crossref_api(query)
            st.session_state.search_results = results
            st.session_state.bias_summary = summary
            st.session_state.is_exact_search = is_exact
            st.session_state.search_page = 1 
            st.session_state.sort_option = "ì •í™•ë„" if is_exact else "Potential"
            if not results: st.error("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    if st.session_state.search_results:
        summary = st.session_state.bias_summary
        
        # 1. í¸í–¥ ìš”ì•½
        with st.expander("ğŸ” í¸í–¥ ìš”ì•½", expanded=True):
            bc1, bc2, bc3 = st.columns(3)
            pub_cnt = summary['pubmed_count']
            pub_cnt_str = f"{pub_cnt:,}í¸" if isinstance(pub_cnt, int) else str(pub_cnt)
            with bc1: st.metric("PubMed ë…¼ë¬¸ ìˆ˜", pub_cnt_str, help="í•´ë‹¹ í‚¤ì›Œë“œì˜ ì „ì²´ ë¬¸í—Œ ìˆ˜ (ì‹œì¥ ê·œëª¨)")
            with bc2: st.metric("í‰ê·  ì¸ìš©ìˆ˜ (Top 200)", f"{summary['avg_citations']:,}íšŒ")
            
            # Multiplier í‘œì‹œ
            mult = summary.get('multiplier', 1.0)
            mult_color = "normal"
            if mult >= 2.0: mult_color = "off" # ë¹¨ê°• ëŠë‚Œ
            elif mult >= 1.5: mult_color = "off"
            
            with bc3: 
                st.metric("ê³¼ì—´ë„ ê°€ì¤‘ì¹˜", f"x{mult}", help="ë¬¸í—ŒëŸ‰ì´ ë§ì„ìˆ˜ë¡ ì¸ìš©ìˆ˜ ê±°í’ˆì„ ì œê±°í•˜ê¸° ìœ„í•´ í˜ë„í‹°ê°€ ê°•í™”ë©ë‹ˆë‹¤.")

            if summary['is_high_exposure']:
                st.warning(f"âš  **High Exposure Topic**: ì—°êµ¬ê°€ ë§¤ìš° í™œë°œí•˜ì—¬(x{mult}), ìƒìœ„ ë…¸ì¶œ ë…¼ë¬¸ì˜ Impact(ì˜í–¥ë ¥)ê°€ ê³¼ëŒ€í‰ê°€ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤. Potential(ì ì¬ë ¥) ì§€í‘œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.")
            else:
                st.success("âœ… **Niche Topic**: ë¹„êµì  ì—°êµ¬ê°€ ëœ ëœ ë¶„ì•¼ì…ë‹ˆë‹¤. ìˆ¨ê²¨ì§„ ëª…ì‘ì´ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        st.divider()

        # 2. ê±°í’ˆ vs ì›ì„ ì‚°ì ë„
        with st.expander("ğŸ“ˆ ê±°í’ˆ vs ì›ì„ ë¶„í¬ë„", expanded=True):
            chart_data = []
            for p in st.session_state.search_results:
                chart_data.append({
                    "Title": p['title'],
                    "Impact": p['raw_score'],
                    "Potential": p['debiased_score'],
                    "Type": p['potential_type']
                })
            
            if chart_data:
                df_chart = pd.DataFrame(chart_data)
                domain = ["amazing", "bubble", "bad", "normal", "uncertain", "suspected", "verified_user"]
                range_ = ["#10b981", "#ef4444", "#6b7280", "#3b82f6", "#f59e0b", "#f59e0b", "#8b5cf6"]
                
                base = alt.Chart(df_chart).encode(
                    x=alt.X('Impact', title='Impact (ì¸ê¸°ë„/ì˜í–¥ë ¥)', scale=alt.Scale(domain=[0, 100])),
                    y=alt.Y('Potential', title='Potential (ì ì¬ë ¥/ë‚´ì‹¤)', scale=alt.Scale(domain=[0, 100]))
                )
                scatter = base.mark_circle(size=60).encode(
                    color=alt.Color('Type', scale=alt.Scale(domain=domain, range=range_), legend=None),
                    tooltip=['Title', 'Impact', 'Potential', 'Type']
                )
                # 4ë¶„ë©´ ê¸°ì¤€ì„ 
                h_rule = alt.Chart(pd.DataFrame({'y': [50]})).mark_rule(strokeDash=[5, 5], color='gray', opacity=0.5).encode(y='y')
                v_rule = alt.Chart(pd.DataFrame({'x': [50]})).mark_rule(strokeDash=[5, 5], color='gray', opacity=0.5).encode(x='x')
                # í…ìŠ¤íŠ¸ ë¼ë²¨
                text_df = pd.DataFrame({
                    'x': [25, 85], 'y': [90, 10], 
                    'label': ['ğŸ’ Hidden Gem (ì›ì„)', 'ğŸ«§ Bubble (ê±°í’ˆ)']
                })
                text_layer = alt.Chart(text_df).mark_text(
                    align='center', baseline='middle', fontSize=13, fontWeight='bold', color='gray', opacity=0.8
                ).encode(x='x', y='y', text='label')
                
                final_chart = (scatter + h_rule + v_rule + text_layer).interactive()
                st.altair_chart(final_chart, use_container_width=True)
                st.info("ğŸ’¡ **ì¢Œì¸¡ ìƒë‹¨(High Potential, Low Impact)** ì˜ì—­ì— ìœ„ì¹˜í•œ ë…¼ë¬¸ì´ ë°”ë¡œ ìˆ¨ê²¨ì§„ ì›ì„(Hidden Gem)ì…ë‹ˆë‹¤!")

        # 3. ì •ë ¬ ë° ëª©ë¡ í‘œì‹œ
        st.markdown("""<div style="font-size: 1rem; font-weight: 600; margin-bottom: 1rem;">ğŸ”ƒ ì •ë ¬ ê¸°ì¤€ ì„ íƒ</div>""", unsafe_allow_html=True)
        sort_col, _ = st.columns([2, 1])
        with sort_col:
            sort_opt = st.radio(
                "ì •ë ¬ ê¸°ì¤€", 
                ["Potential (ì ì¬ë ¥)", "Impact (ì˜í–¥ë ¥)", "ìµœì‹ ", "ì •í™•ë„"], 
                horizontal=True, 
                label_visibility="collapsed", 
                key="sort_selector"
            )
        
        if "Potential" in sort_opt:
            st.session_state.search_results.sort(key=lambda x: x['debiased_score'], reverse=True)
        elif "Impact" in sort_opt:
            st.session_state.search_results.sort(key=lambda x: x['raw_score'], reverse=True)
        elif sort_opt == "ìµœì‹ ":
            st.session_state.search_results.sort(key=lambda x: x['year'], reverse=True)
        elif sort_opt == "ì •í™•ë„":
            st.session_state.search_results.sort(key=lambda x: x['original_rank'])

        # í˜ì´ì§€ë„¤ì´ì…˜
        items_per_page = 50
        total_items = len(st.session_state.search_results)
        total_pages = max(1, math.ceil(total_items / items_per_page))
        current_page = st.session_state.search_page
        start_idx = (current_page - 1) * items_per_page
        end_idx = start_idx + items_per_page
        page_items = st.session_state.search_results[start_idx:end_idx]

        st.caption(f"ê²€ìƒ‰ ê²°ê³¼ ì´ {total_items}ê±´ | ì •ë ¬: {sort_opt} | í˜ì´ì§€: {current_page}/{total_pages}")
        
        for i, paper in enumerate(page_items):
            unique_key_idx = start_idx + i
            with st.container(border=True):
                c1, c2 = st.columns([5, 2])
                with c1:
                    translated_title = get_translated_title(paper['title'])
                    display_title = highlight_text(paper['title']) if show_highlight else paper['title']
                    st.markdown(
                        f"""<div title="[ë²ˆì—­] {translated_title}" style="font-size:1.1rem; font-weight:bold; margin-bottom:5px;">{start_idx + i + 1}. {display_title}</div>""", 
                        unsafe_allow_html=True
                    )
                    if show_translation:
                        st.caption(f"ğŸ‡°ğŸ‡· {translated_title}")
                    
                    tags = []
                    if paper['has_evidence']: tags.append("ğŸ”¬ Evidence")
                    if paper['is_big_team']: tags.append("ğŸ‘¥ Big Team")
                    if paper['integrity_status'] != "valid": tags.append("âš ï¸ ë°ì´í„° ë¶€ì¡±")
                    if paper['potential_type'] == "amazing": tags.append("ğŸ’ Hidden Gem")
                    st.write(" ".join([f"`{t}`" for t in tags]))
                    
                    auth_display = ", ".join(paper['authors'])
                    if paper['author_full_count'] > 3: auth_display += f" ì™¸ {paper['author_full_count'] - 3}ëª…"
                    st.caption(f"{paper['year']} | {paper['journal']} | ì¸ìš© {paper['citations']}íšŒ | ì €ì: {auth_display}")
                    st.markdown(f"[ğŸ“„ ì›ë¬¸ ë³´ê¸°]({paper['url']})")

                with c2:
                    col_raw, col_deb = st.columns(2)
                    with col_raw: st.metric("Impact", f"{paper['raw_score']}", help="í˜„ì¬ í•™ê³„ì—ì„œì˜ ì˜í–¥ë ¥ ë° ì¸ê¸°ë„ (Raw Score)")
                    with col_deb: st.metric("Potential", f"{paper['debiased_score']}", delta=f"{-paper['bias_penalty']}", help="ë¯¸ë˜ ê°€ì¹˜ ë° ì ì¬ë ¥ (Debiased Score)")
                    if paper['bias_penalty'] > 20: st.caption("âš  ê³¼ì—´ë¨")

                    is_owned = any(p['id'] == paper['id'] for p in st.session_state.inventory)
                    if is_owned:
                        st.button("ë³´ìœ ì¤‘", key=f"owned_{unique_key_idx}", disabled=True, use_container_width=True)
                    else:
                        if st.button("ìˆ˜ì§‘", key=f"collect_{unique_key_idx}", type="secondary", use_container_width=True):
                            st.session_state.inventory.append(paper)
                            st.session_state.score += paper['debiased_score']
                            save_user_data(st.session_state.user_id) 
                            st.rerun()
        
        st.divider()
        # í˜ì´ì§€ ì´ë™ ë²„íŠ¼
        _, nav_col, _ = st.columns([1, 5, 1])
        with nav_col:
            if total_pages <= 5: display_pages = range(1, total_pages + 1)
            else:
                if current_page <= 3: display_pages = range(1, 6)
                elif current_page >= total_pages - 2: display_pages = range(total_pages - 4, total_pages + 1)
                else: display_pages = range(current_page - 2, current_page + 3)

            pg_cols = st.columns([1, 1, 1, 1, 1, 1, 1, 0.5, 2.5], gap="small")
            with pg_cols[0]:
                if st.button("â—€", key="nav_prev", disabled=current_page==1, use_container_width=True):
                    st.session_state.search_page -= 1
                    st.rerun()
            for idx, p_num in enumerate(display_pages):
                if idx < 5:
                    with pg_cols[idx + 1]:
                        b_type = "primary" if p_num == current_page else "secondary"
                        if st.button(f"{p_num}", key=f"nav_p_{p_num}", type=b_type, use_container_width=True):
                            st.session_state.search_page = p_num
                            st.rerun()
            with pg_cols[6]:
                if st.button("â–¶", key="nav_next", disabled=current_page==total_pages, use_container_width=True):
                    st.session_state.search_page += 1
                    st.rerun()
            with pg_cols[8]:
                 new_page = st.number_input("ì´ë™", min_value=1, max_value=total_pages, value=current_page, label_visibility="collapsed", key="nav_search_input")
                 if new_page != current_page:
                    st.session_state.search_page = new_page
                    st.rerun()

# ------------------------------------------------------------------------------
# [Tab 2] ì§€í‘œ ë¶„ì„
# ------------------------------------------------------------------------------
with tab_analysis:
    if not st.session_state.search_results:
        st.info("ë¨¼ì € 'ë…¼ë¬¸ ê²€ìƒ‰' íƒ­ì—ì„œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.")
    else:
        st.markdown("""<div style="font-size: 1.5rem; font-weight: 600; margin-bottom: 1rem;">ğŸ› ï¸ ë§ì¶¤í˜• ì§€í‘œ ë¶„ì„</div>""", unsafe_allow_html=True)
        st.markdown("ê° ì§€í‘œì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•˜ì—¬ ë‚˜ë§Œì˜ ê¸°ì¤€(Custom Potential)ìœ¼ë¡œ ë…¼ë¬¸ì„ ì¬í‰ê°€í•˜ê³  ì •ë ¬í•©ë‹ˆë‹¤.")
        
        if 'analysis_weights' not in st.session_state:
            st.session_state.analysis_weights = {"evidence": 1.0, "recency": 1.0, "team": 1.0, "scarcity": 1.0}
            st.session_state.current_preset = "âš–ï¸ ë°¸ëŸ°ìŠ¤"
        
        if 'analysis_page' not in st.session_state:
            st.session_state.analysis_page = 1

        col_p1, col_p2, col_p3, col_p4 = st.columns(4)
        with col_p1:
            if st.button("âš–ï¸ ë°¸ëŸ°ìŠ¤", use_container_width=True, help="ëª¨ë“  ì§€í‘œë¥¼ ê³¨ê³ ë£¨ ë°˜ì˜í•©ë‹ˆë‹¤."):
                st.session_state.analysis_weights = {"evidence": 1.0, "recency": 1.0, "team": 1.0, "scarcity": 1.0}
                st.session_state.current_preset = "âš–ï¸ ë°¸ëŸ°ìŠ¤"
                st.rerun()
        with col_p2:
            if st.button("ğŸ’ ìˆ¨ê²¨ì§„ ì›ì„", use_container_width=True, help="ì¸ìš©ì€ ì ì§€ë§Œ ì¦ê±°ê°€ í™•ì‹¤í•œ ë…¼ë¬¸ì„ ì°¾ìŠµë‹ˆë‹¤."):
                st.session_state.analysis_weights = {"evidence": 2.0, "recency": 1.0, "team": 1.0, "scarcity": 3.0}
                st.session_state.current_preset = "ğŸ’ ìˆ¨ê²¨ì§„ ì›ì„"
                st.rerun()
        with col_p3:
            if st.button("ğŸš€ ìµœì‹  íŠ¸ë Œë“œ", use_container_width=True, help="ìµœì‹ ì„±ê³¼ ì‹¤í—˜ì  ê·¼ê±°ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ë´…ë‹ˆë‹¤."):
                st.session_state.analysis_weights = {"evidence": 2.0, "recency": 3.0, "team": 0.5, "scarcity": 1.0}
                st.session_state.current_preset = "ğŸš€ ìµœì‹  íŠ¸ë Œë“œ"
                st.rerun()
        with col_p4:
            if st.button("ğŸ‘‘ ëŒ€ê·œëª¨", use_container_width=True, help="ëŒ€ê·œëª¨ ì—°êµ¬íŒ€ì„ ì„ í˜¸í•©ë‹ˆë‹¤."):
                st.session_state.analysis_weights = {"evidence": 1.0, "recency": 0.5, "team": 3.0, "scarcity": 0.5}
                st.session_state.current_preset = "ğŸ‘‘ ëŒ€ê·œëª¨"
                st.rerun()

        st.info(f"í˜„ì¬ ì ìš©ëœ ë¶„ì„ ëª¨ë“œ: **{st.session_state.current_preset}**")

        st.markdown("""
        <small>ğŸ’¡ **ê°€ì¤‘ì¹˜ ì„¤ì • ê°€ì´ë“œ**: ìŠ¬ë¼ì´ë”ì˜ ìˆ«ìëŠ” í•´ë‹¹ ì§€í‘œì˜ ì¤‘ìš”ë„(ë°°ìˆ˜)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
        <br>â€¢ **1.0**: ê¸°ë³¸ ë°˜ì˜ | â€¢ **2.0**: 2ë°° ë” ì¤‘ìš”í•˜ê²Œ ë°˜ì˜ | â€¢ **0.0**: ì ìˆ˜ ì‚°ì •ì—ì„œ ì œì™¸</small>
        """, unsafe_allow_html=True)

        w = st.session_state.analysis_weights
        
        with st.container(border=True):
            col_w1, col_w2 = st.columns(2)
            with col_w1: w["evidence"] = st.slider("ì¦ê±°", 0.0, 3.0, w["evidence"])
            with col_w2: w["recency"] = st.slider("ìµœì‹ ì„±", 0.0, 3.0, w["recency"])
            col_w3, col_w4 = st.columns(2)
            with col_w3: w["team"] = st.slider("ê·œëª¨", 0.0, 3.0, w["team"])
            with col_w4: w["scarcity"] = st.slider("í¬ì†Œì„±", 0.0, 3.0, w["scarcity"])

        w_evidence = w["evidence"]
        w_recency = w["recency"]
        w_team = w["team"]
        w_scarcity = w["scarcity"]

        analyzed_papers = []
        for paper in st.session_state.search_results:
            details = paper.get('score_breakdown', {})
            ev_score = details.get('Evidence', 0)
            team_score = details.get('Team', 0)
            vol_penalty = details.get('Volume Penalty', 0)
            age_score = max(0, (5 - paper.get('age', 5)) * 10)
            scarcity_score = max(0, (50 - paper.get('citation_count', 0))) 
            if scarcity_score > 50: scarcity_score = 50
            
            custom_score = (
                (ev_score * w_evidence) +
                (team_score * w_team) +
                (age_score * w_recency) +
                (scarcity_score * w_scarcity) +
                vol_penalty
            )
            paper_copy = paper.copy()
            paper_copy['custom_score'] = int(custom_score)
            analyzed_papers.append(paper_copy)
            
        analyzed_papers.sort(key=lambda x: x['custom_score'], reverse=True)
        
        # ë¶„ì„ íƒ­ í˜ì´ì§€ë„¤ì´ì…˜
        items_per_page = 50
        total_items_an = len(analyzed_papers)
        total_pages_an = max(1, math.ceil(total_items_an / items_per_page))
        current_page_an = st.session_state.analysis_page
        start_idx_an = (current_page_an - 1) * items_per_page
        end_idx_an = start_idx_an + items_per_page
        page_items_an = analyzed_papers[start_idx_an:end_idx_an]

        st.divider()
        st.caption(f"ì¬í‰ê°€ ê²°ê³¼ ({total_items_an}ê±´) | í˜ì´ì§€: {current_page_an}/{total_pages_an}")
        
        for i, paper in enumerate(page_items_an):
            unique_an_key = f"an_{start_idx_an + i}"
            with st.container(border=True):
                c1, c2 = st.columns([5, 2])
                with c1:
                    translated_title = get_translated_title(paper['title'])
                    display_title = highlight_text(paper['title']) if show_highlight else paper['title']
                    st.markdown(
                        f"""<div title="[ë²ˆì—­] {translated_title}" style="font-size:1.1rem; font-weight:bold; margin-bottom:5px;">{display_title}</div>""", 
                        unsafe_allow_html=True
                    )
                    if show_translation:
                        st.caption(f"ğŸ‡°ğŸ‡· {translated_title}")
                    
                    tags = []
                    if paper['has_evidence']: tags.append("ğŸ”¬ Evidence")
                    if paper['is_big_team']: tags.append("ğŸ‘¥ Big Team")
                    if paper['integrity_status'] != "valid": tags.append("âš ï¸ ë°ì´í„° ë¶€ì¡±")
                    if paper['potential_type'] == "amazing": tags.append("ğŸ’ Hidden Gem")
                    st.write(" ".join([f"`{t}`" for t in tags]))
                    
                    auth_display = ", ".join(paper['authors'])
                    if paper['author_full_count'] > 3: auth_display += f" ì™¸ {paper['author_full_count'] - 3}ëª…"
                    st.caption(f"{paper['year']} | {paper['journal']} | ì¸ìš© {paper['citations']}íšŒ | ì €ì: {auth_display}")
                    st.markdown(f"[ğŸ“„ ì›ë¬¸ ë³´ê¸°]({paper['url']})")

                    with st.expander("ì ìˆ˜ ìƒì„¸ êµ¬ì„± ë³´ê¸°"):
                        details = paper.get('score_breakdown', {})
                        chart_data = {
                            "Evidence (ì¦ê±°)": details.get('Evidence', 0) * w_evidence,
                            "Team (ê·œëª¨)": details.get('Team', 0) * w_team,
                            "Recency (ìµœì‹ ì„±)": max(0, (5 - paper.get('age', 5)) * 10) * w_recency,
                            "Scarcity (í¬ì†Œì„±)": max(0, (50 - paper.get('citation_count', 0))) * w_scarcity,
                        }
                        st.bar_chart(chart_data, horizontal=True)
                with c2:
                    st.metric("ì‚¬ìš©ì ì ìˆ˜", f"{paper['custom_score']}")
                    is_owned = any(p['id'] == paper['id'] for p in st.session_state.inventory)
                    if is_owned:
                        st.button("ë³´ìœ ì¤‘", key=f"an_own_{unique_an_key}", disabled=True, use_container_width=True)
                    else:
                        if st.button("ìˆ˜ì§‘", key=f"an_col_{unique_an_key}", type="secondary", use_container_width=True):
                            st.session_state.inventory.append(paper)
                            st.session_state.score += paper['debiased_score']
                            save_user_data(st.session_state.user_id)
                            st.rerun()

        st.divider()
        _, nav_col_an, _ = st.columns([1, 5, 1])
        with nav_col_an:
            if total_pages_an <= 5: display_pages_an = range(1, total_pages_an + 1)
            else:
                if current_page_an <= 3: display_pages_an = range(1, 6)
                elif current_page_an >= total_pages_an - 2: display_pages_an = range(total_pages_an - 4, total_pages_an + 1)
                else: display_pages_an = range(current_page_an - 2, current_page_an + 3)

            pg_cols_an = st.columns([1, 1, 1, 1, 1, 1, 1, 0.5, 2.5], gap="small")
            
            with pg_cols_an[0]:
                if st.button("â—€", key="nav_an_prev", disabled=current_page_an==1, use_container_width=True):
                    st.session_state.analysis_page -= 1
                    st.rerun()
            for idx, p_num in enumerate(display_pages_an):
                if idx < 5:
                    with pg_cols_an[idx + 1]:
                        b_type = "primary" if p_num == current_page_an else "secondary"
                        if st.button(f"{p_num}", key=f"nav_an_p_{p_num}", type=b_type, use_container_width=True):
                            st.session_state.analysis_page = p_num
                            st.rerun()
            with pg_cols_an[6]:
                if st.button("â–¶", key="nav_an_next", disabled=current_page_an==total_pages_an, use_container_width=True):
                    st.session_state.analysis_page += 1
                    st.rerun()
            with pg_cols_an[8]:
                 new_page_an = st.number_input("ì´ë™", min_value=1, max_value=total_pages_an, value=current_page_an, label_visibility="collapsed", key="nav_an_input")
                 if new_page_an != current_page_an:
                    st.session_state.analysis_page = new_page_an
                    st.rerun()

# ------------------------------------------------------------------------------
# [Tab 3] ë‚´ ì„œì¬ (Inventory)
# ------------------------------------------------------------------------------
with tab_inventory:
    inv_main, inv_info = st.columns([3, 1])
    
    with inv_info:
        with st.container(border=True):
            st.markdown("""<div style="font-size: 1.2rem; font-weight: 600; margin-bottom: 1rem;">ğŸ’¡ ê°€ì¹˜ ì‚°ì • ê³µì‹</div>""", unsafe_allow_html=True)
            st.markdown("""
            **1. ì‹¬ì¸µ ê²€ì¦ (ì„±ê³µ)**
            > **Potential + 50% ë³´ë„ˆìŠ¤**

            <small>ì¢‹ì€ ì›ì„(Potential)ì„ ë°œêµ´í• ìˆ˜ë¡, ì—°êµ¬ìì˜ ê²€ì¦ì„ í†µí•´ ê·¸ ê°€ì¹˜ê°€ 1.5ë°°ë¡œ ì¦í­ë©ë‹ˆë‹¤.</small>
            """, unsafe_allow_html=True)
            st.markdown("---")
            st.markdown("""
            **2. ê°•ì œ ìŠ¹ì¸ (ë¦¬ìŠ¤í¬)**
            > **Potential + 10ì **

            <small>ë°ì´í„°ê°€ ë¶€ì¡±í•œ(Risk) ë…¼ë¬¸ì„ ì–µì§€ë¡œ ìŠ¹ì¸í•˜ë©´, ë³´ë„ˆìŠ¤ê°€ ëŒ€í­ ì¶•ì†Œë©ë‹ˆë‹¤.</small>
            """, unsafe_allow_html=True)

    with inv_main:
        if not st.session_state.inventory: 
            st.info("ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            with st.expander("ğŸ“‚ ì„œì§€ ì •ë³´ ë‚´ë³´ë‚´ê¸° (BibTeX / CSV)"):
                e_col1, e_col2 = st.columns(2)
                with e_col1:
                    bib_data = convert_to_bibtex(st.session_state.inventory)
                    st.download_button("BibTeX ë‹¤ìš´ë¡œë“œ (.bib)", bib_data, "my_research_inventory.bib", "text/plain", use_container_width=True)
                with e_col2:
                    csv_data = convert_to_csv(st.session_state.inventory)
                    st.download_button("CSV ë‹¤ìš´ë¡œë“œ (.csv)", csv_data, "my_research_inventory.csv", "text/csv", use_container_width=True)
            
            with st.expander("ğŸ“– Overleafë¡œ BibTeX ì“°ëŠ” ì´ˆê°„ë‹¨ ë£¨íŠ¸ (ê°€ì´ë“œ ë³´ê¸°)"):
                st.markdown("ğŸ”— [Overleaf ë¡œê·¸ì¸ ë°”ë¡œê°€ê¸°](https://www.overleaf.com/login)")
                st.markdown(r"""
                BibTeXì—ì„œ .bib íŒŒì¼ì€ ì°¸ê³ ë¬¸í—Œì´ â€œì¶œë ¥ëœ ê²°ê³¼ë¬¼â€ì´ ì•„ë‹ˆë¼, ë…¼ë¬¸ ì •ë³´ê°€ ì •ë¦¬ë˜ì–´ ìˆëŠ” ë°ì´í„° íŒŒì¼ì— í•´ë‹¹í•œë‹¤. ê·¸ë˜ì„œ .bib íŒŒì¼ì„ ê·¸ëƒ¥ ì—´ì–´ì„œëŠ” ì°¸ê³ ë¬¸í—Œ ëª©ë¡ì´ ë³´ì´ì§€ ì•Šê³ , ë°˜ë“œì‹œ LaTeX ë¬¸ì„œê°€ ì´ íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ PDFë¡œ ì¶œë ¥í•´ ì£¼ì–´ì•¼ í•œë‹¤. Overleafë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ì´ ê³¼ì •ì„ ê°€ì¥ ê°„ë‹¨í•˜ê²Œ ì²˜ë¦¬í•´ ì£¼ê¸° ë•Œë¬¸ì´ë‹¤.

                Overleafì—ì„œëŠ” ë¨¼ì € ìƒˆ í”„ë¡œì íŠ¸ë¥¼ ë§Œë“¤ê³ , ê¸°ë³¸ìœ¼ë¡œ ìƒì„±ëœ main.tex íŒŒì¼ê³¼ í•¨ê»˜ ê°€ì§€ê³  ìˆëŠ” .bib íŒŒì¼ì„ ê°™ì€ í”„ë¡œì íŠ¸ ì•ˆì— ì—…ë¡œë“œí•œë‹¤. ê·¸ë‹¤ìŒ main.texì—ì„œ ë³¸ë¬¸ì„ ì‘ì„±í•˜ê³ , ë¬¸ì„œì˜ ëë¶€ë¶„, ì¦‰ `\end{document}` ë°”ë¡œ ìœ„ì— BibTeX ê´€ë ¨ ì½”ë“œë¥¼ ì¶”ê°€í•œë‹¤. ì´ë•Œ `\bibliography{references}`ëŠ” â€œreferences.bibë¼ëŠ” íŒŒì¼ì„ ì°¸ê³ ë¬¸í—Œ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ê² ë‹¤â€ëŠ” ì˜ë¯¸ì´ê³ , í™•ì¥ì .bibëŠ” ì“°ì§€ ì•ŠëŠ”ë‹¤. ë§Œì•½ .bib íŒŒì¼ ì•ˆì— ë“¤ì–´ ìˆëŠ” ëª¨ë“  ë…¼ë¬¸ì„ í•œêº¼ë²ˆì— ì°¸ê³ ë¬¸í—Œìœ¼ë¡œ ì¶œë ¥í•˜ê³  ì‹¶ë‹¤ë©´ `\nocite{*}`ë¥¼ í•¨ê»˜ ë„£ì–´ ì£¼ë©´ ëœë‹¤.

                ì—¬ê¸°ì„œ `\bibliographystyle{unsrt}`ëŠ” ì°¸ê³ ë¬¸í—Œì˜ ì¶œë ¥ í˜•ì‹ê³¼ ì •ë ¬ ë°©ì‹ì„ ì§€ì •í•˜ëŠ” ì—­í• ì„ í•œë‹¤. unsrtëŠ” â€œì •ë ¬í•˜ì§€ ì•ŠëŠ”ë‹¤(unsorted)â€ëŠ” ëœ»ìœ¼ë¡œ, ë³¸ë¬¸ì—ì„œ ì¸ìš©ëœ ìˆœì„œ ê·¸ëŒ€ë¡œ ì°¸ê³ ë¬¸í—Œì„ ë‚˜ì—´í•˜ë¼ëŠ” ì˜ë¯¸ë‹¤. ì¦‰, ì„œë¡ ì—ì„œ ì²˜ìŒ ì¸ìš©í•œ ë…¼ë¬¸ì´ 1ë²ˆ, ê·¸ë‹¤ìŒì— ì¸ìš©í•œ ë…¼ë¬¸ì´ 2ë²ˆì´ ë˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ ë°©ì‹ì€ ìì—°ê³¼í•™, ì˜ìƒëª… ë¶„ì•¼ ë…¼ë¬¸ì´ë‚˜ ìº¡ìŠ¤í†¤ ë³´ê³ ì„œì—ì„œ ê°€ì¥ í”íˆ ì“°ì´ë©°, ë…ìê°€ ë³¸ë¬¸ íë¦„ì„ ë”°ë¼ê°€ë©´ì„œ ì°¸ê³ ë¬¸í—Œì„ í™•ì¸í•˜ê¸° ì‰½ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.

                ì´ë ‡ê²Œ .bib íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³ , ë¬¸ì„œ ë§¨ ì•„ë˜ì— `\bibliographystyle{unsrt}`ì™€ `\bibliography{bib íŒŒì¼ ì´ë¦„}`ë¥¼ ì¶”ê°€í•œ ë’¤ Recompile ë²„íŠ¼ì„ ëˆ„ë¥´ë©´, Overleafê°€ LaTeXì™€ BibTeXë¥¼ ìë™ìœ¼ë¡œ ì‹¤í–‰í•´ ì£¼ê³  PDFì— ì°¸ê³ ë¬¸í—Œ ëª©ë¡ì„ ë§Œë“¤ì–´ ì¤€ë‹¤. ì‚¬ìš©ìëŠ” ì»´íŒŒì¼ ìˆœì„œë¥¼ ì‹ ê²½ ì“¸ í•„ìš”ê°€ ì—†ê³ , íŒŒì¼ ì´ë¦„ë§Œ ì •í™•íˆ ë§ì¶”ë©´ ëœë‹¤.

                ì •ë¦¬í•˜ë©´, Overleafì—ì„œ BibTeXë¥¼ ì“°ëŠ” í•µì‹¬ì€ â€œ.bib íŒŒì¼ì€ ë°ì´í„°, .tex íŒŒì¼ì€ ì´ë¥¼ ì¶œë ¥í•˜ëŠ” ë„êµ¬â€ë¼ëŠ” ì ì„ ì´í•´í•˜ê³ , ë¬¸ì„œ ëì— ì°¸ê³ ë¬¸í—Œ ìŠ¤íƒ€ì¼ê³¼ ë°ì´í„° íŒŒì¼ì„ ì§€ì •í•´ ì£¼ëŠ” ê²ƒì´ë‹¤. `\bibliographystyle{unsrt}`ëŠ” ê·¸ì¤‘ì—ì„œë„ â€œì°¸ê³ ë¬¸í—Œì„ ì–´ë–¤ ê·œì¹™ìœ¼ë¡œ ë³´ì—¬ì¤„ì§€â€ë¥¼ ì •í•˜ëŠ” ì¤‘ìš”í•œ í•œ ì¤„ì´ë¼ê³  ë³´ë©´ ëœë‹¤.
                """)
                st.code(r"""
\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\title{Title}
\author{Name}
\date{Month Year}
\begin{document}
\maketitle
\section{Introduction}

#ì•„ë˜ 3ì¤„ ë³µì‚¬ ë¶™ì—¬ë„£ê¸°
======================
\nocite{*}
\bibliographystyle{unsrt}
\bibliography{bibíŒŒì¼ ì´ë¦„}
======================

\end{document}
""", language="latex")

            st.divider()
            col_sort, _ = st.columns([2, 5])
            with col_sort:
                inv_sort_opt = st.selectbox("ì •ë ¬ ë°©ì‹", ["ì €ì¥í•œ ìˆœì„œ", "ê°€ì¹˜ ë†’ì€ ìˆœì„œ"])
            
            inv_list = st.session_state.inventory
            if inv_sort_opt == "ê°€ì¹˜ ë†’ì€ ìˆœì„œ":
                display_items = sorted(inv_list, key=lambda x: x.get('final_score', x.get('debiased_score', 0)), reverse=True)
            else:
                display_items = inv_list

            for i, paper in enumerate(display_items):
                p_id = paper['id']
                with st.container(border=True):
                    c1, c2 = st.columns([5, 2])
                    
                    # Left Column: Paper Info & Chart (Same as Search Tab)
                    with c1:
                        # Title
                        translated_title = get_translated_title(paper['title'])
                        display_title = highlight_text(paper['title']) if show_highlight else paper['title']
                        st.markdown(
                            f"""<div title="[ë²ˆì—­] {translated_title}" style="font-size:1.2rem; font-weight:bold; margin-bottom:5px;">{display_title}</div>""", 
                            unsafe_allow_html=True
                        )
                        if show_translation:
                            st.caption(f"ğŸ‡°ğŸ‡· {translated_title}")
                        
                        # Tags
                        tags = []
                        if paper['has_evidence']: tags.append("ğŸ”¬ Evidence")
                        if paper['is_big_team']: tags.append("ğŸ‘¥ Big Team")
                        if paper['integrity_status'] != "valid": tags.append("âš ï¸ ë°ì´í„° ë¶€ì¡±")
                        if paper['potential_type'] == "amazing": tags.append("ğŸ’ Hidden Gem")
                        st.write(" ".join([f"`{t}`" for t in tags]))
                        
                        # Meta Info
                        auth_display = ", ".join(paper['authors'])
                        if paper['author_full_count'] > 3: auth_display += f" ì™¸ {paper['author_full_count'] - 3}ëª…"
                        st.caption(f"{paper['year']} | {paper['journal']} | ì¸ìš© {paper['citations']}íšŒ | ì €ì: {auth_display}")
                        st.markdown(f"[ğŸ“„ ì›ë¬¸ ë³´ê¸°]({paper['url']})")

                        # Chart
                        with st.expander("ì ìˆ˜ ìƒì„¸ êµ¬ì„± ë³´ê¸°"):
                            details = paper.get('score_breakdown', {})
                            w_evidence = st.session_state.analysis_weights["evidence"]
                            w_team = st.session_state.analysis_weights["team"]
                            w_recency = st.session_state.analysis_weights["recency"]
                            w_scarcity = st.session_state.analysis_weights["scarcity"]
                            
                            chart_data = {
                                "Evidence (ì¦ê±°)": details.get('Evidence', 0) * w_evidence,
                                "Team (ê·œëª¨)": details.get('Team', 0) * w_team,
                                "Recency (ìµœì‹ ì„±)": max(0, (5 - paper.get('age', 5)) * 10) * w_recency,
                                "Scarcity (í¬ì†Œì„±)": max(0, (50 - paper.get('citation_count', 0))) * w_scarcity,
                            }
                            st.bar_chart(chart_data, horizontal=True)

                    # Right Column: Metrics & Actions (Inventory Specific)
                    with c2:
                        # Base Metrics
                        col_raw, col_deb = st.columns(2)
                        # [Fixed] Safe access to dictionary keys
                        raw_s = paper.get('raw_score', 0)
                        deb_s = paper.get('debiased_score', 0)
                        bias_p = paper.get('bias_penalty', 0)
                        
                        with col_raw: st.metric("Impact", f"{raw_s}", help="í˜„ì¬ í•™ê³„ì—ì„œì˜ ì˜í–¥ë ¥")
                        with col_deb: st.metric("Potential", f"{deb_s}", delta=f"{-bias_p}", help="ë¯¸ë˜ ê°€ì¹˜")
                        if bias_p > 20: st.caption("âš  ê³¼ì—´ë¨")
                        
                        st.divider()
                        
                        # Validation Status & Value
                        if paper['is_reviewed']:
                            status_emoji = "âœ…"
                            if paper['potential_type'] == "amazing": status_emoji = "âœ¨ ëŒ€ì„±ê³µ"
                            elif paper['potential_type'] == "bad": status_emoji = "ğŸ’€ ì‹¤íŒ¨"
                            elif paper['potential_type'] == "verified_user": status_emoji = "ğŸ›¡ï¸ ì‚¬ìš©ì ìŠ¹ì¸"
                            
                            st.success(f"{status_emoji} (ìµœì¢…: {paper.get('final_score', 0)}ì )")
                        else:
                            # Action Buttons for Unreviewed
                            if paper['integrity_status'] == "valid":
                                if st.button("ğŸ”¬ ì‹¬ì¸µ ê²€ì¦", key=f"rev_{p_id}", type="primary", use_container_width=True):
                                    paper['is_reviewed'] = True
                                    bonus = int(deb_s * 0.5)
                                    st.session_state.score += bonus
                                    paper['final_score'] = deb_s + bonus
                                    if paper['potential_type'] == 'amazing': st.toast(f"ëŒ€ë°•! ìˆ¨ê²¨ì§„ ëª…ì‘ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤! (+{bonus})", icon="ğŸ‰")
                                    else: st.toast(f"ê²€ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (+{bonus})", icon="âœ…")
                                    save_user_data(st.session_state.user_id) 
                                    st.rerun()
                            else:
                                st.warning(paper['risk_reason'])
                                if st.button("ê°•ì œ ìŠ¹ì¸", key=f"force_{p_id}", use_container_width=True):
                                    paper['is_reviewed'] = True
                                    bonus = 10 
                                    st.session_state.score += bonus
                                    paper['final_score'] = deb_s + bonus
                                    paper['potential_type'] = "verified_user"
                                    paper['reason'] = "ì‚¬ìš©ì ì§ì ‘ í™•ì¸ìœ¼ë¡œ ê²€ì¦ë¨"
                                    save_user_data(st.session_state.user_id) 
                                    st.rerun()
                        
                        # Delete Button
                        if st.button("ì‚­ì œ", key=f"del_{p_id}", use_container_width=True):
                            deduction = paper.get('final_score', deb_s)
                            st.session_state.score = max(0, st.session_state.score - deduction)
                            st.session_state.inventory = [p for p in st.session_state.inventory if p['id'] != p_id]
                            st.session_state.trash.append(paper)
                            st.toast(f"ë…¼ë¬¸ ì‚­ì œ. {deduction}ì  ì°¨ê°ë¨", icon="ğŸ—‘ï¸")
                            save_user_data(st.session_state.user_id) 
                            st.rerun()

# ------------------------------------------------------------------------------
# [Tab 4] íœ´ì§€í†µ (Trash)
# ------------------------------------------------------------------------------
with tab_trash:
    if not st.session_state.trash: st.info("íœ´ì§€í†µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    if st.session_state.trash:
        if st.button("íœ´ì§€í†µ ë¹„ìš°ê¸°", type="primary"):
            st.session_state.trash = []
            save_user_data(st.session_state.user_id)
            st.toast("íœ´ì§€í†µ ë¹„ì›€", icon="ğŸ§¹")
            st.rerun()
    cols = st.columns(2)
    for i, paper in enumerate(st.session_state.trash):
        with cols[i % 2]:
            with st.container(border=True):
                translated_title = get_translated_title(paper['title'])
                display_title = highlight_text(paper['title']) if show_highlight else paper['title']
                st.markdown(
                    f"""<div title="[ë²ˆì—­] {translated_title}" style="font-size:1rem; font-weight:bold; color:gray; margin-bottom:5px;">{display_title}</div>""", 
                    unsafe_allow_html=True
                )
                if show_translation: st.caption(f"ğŸ‡°ğŸ‡· {translated_title}")
                st.caption(f"ì‚­ì œë¨ | {paper['journal']}")
                
                c1, c2 = st.columns(2)
                with c1:
                    if st.button("ë³µêµ¬", key=f"rest_{i}", use_container_width=True):
                        restored = st.session_state.trash.pop(i)
                        st.session_state.inventory.append(restored)
                        r_score = restored.get('final_score', restored.get('debiased_score', 0))
                        st.session_state.score += r_score
                        st.toast(f"ë³µêµ¬ ì™„ë£Œ (+{r_score}ì )", icon="â™»ï¸")
                        save_user_data(st.session_state.user_id)
                        st.rerun()
                with c2:
                    if st.button("ì˜êµ¬ ì‚­ì œ", key=f"pdel_{i}", use_container_width=True):
                        st.session_state.trash.pop(i)
                        st.toast("ì˜êµ¬ ì‚­ì œë¨", icon="ğŸ”¥")
                        save_user_data(st.session_state.user_id)
                        st.rerun()
