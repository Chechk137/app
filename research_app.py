import streamlit as st
import requests
import datetime
import random
import time
import json
import os
import math
import re
from collections import Counter

# --- 1. ì„¤ì • ë° ìƒìˆ˜ ---

# [New] í•˜ì´ë¼ì´íŒ… ë° í‰ê°€ì— ì‚¬ìš©í•  í•µì‹¬ í‚¤ì›Œë“œ ìƒìˆ˜í™”
EVIDENCE_KEYWORDS = [
    'in vivo', 'in vitro', 'randomized', 'efficacy', 'mechanism', 'signaling', 
    'experiment', 'analysis', 'clinical', 'activity', 'synthesis', 'design', 
    'evaluation', 'characterization', 'properties', 'performance', 'application'
]

MISSIONS = [
    {"id": 1, "text": "ì¸ìš© 100íšŒ ì´ìƒ ë…¼ë¬¸ 1í¸ ìˆ˜ì§‘", "type": "citation", "target": 100, "count": 1, "reward": 150},
    {"id": 2, "text": "5ì¸ ì´ìƒ í˜‘ì—… ì—°êµ¬ ìˆ˜ì§‘", "type": "team", "target": 5, "count": 1, "reward": 100},
    {"id": 3, "text": "í•¨ì • ë…¼ë¬¸ í”¼í•˜ê¸° (ê²€ì¦ ì‹¤íŒ¨ 0íšŒ)", "type": "avoid_trap", "target": "trap", "count": 0, "reward": 0},
    {"id": 4, "text": "ì—°êµ¬ ì ìˆ˜ 1500ì  ë‹¬ì„±", "type": "score", "target": 1500, "count": 1500, "reward": 500},
]

DATA_DIR = "user_data"
if not os.path.exists(DATA_DIR):
    os.makedirs(DATA_DIR)

# --- 2. ë°ì´í„° ê´€ë¦¬ í•¨ìˆ˜ ---

def load_user_data(user_id):
    file_path = os.path.join(DATA_DIR, f"{user_id}.json")
    if os.path.exists(file_path):
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                data = json.load(f)
                return {
                    "score": data.get("score", 0),
                    "inventory": data.get("inventory", []),
                    "mission_id": data.get("mission_id", 1),
                    "trash": data.get("trash", [])
                }
        except Exception as e:
            st.error(f"ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {e}")
    return {"score": 0, "inventory": [], "mission_id": 1, "trash": []}

def save_user_data(user_id):
    file_path = os.path.join(DATA_DIR, f"{user_id}.json")
    data = {
        "score": st.session_state.score,
        "inventory": st.session_state.inventory,
        "mission_id": st.session_state.mission_id,
        "trash": st.session_state.trash
    }
    try:
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    except Exception as e:
        st.error(f"ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {e}")

# --- 3. í•µì‹¬ ë¡œì§ í•¨ìˆ˜ ---

def get_current_year():
    return datetime.datetime.now().year

def get_pubmed_count(query):
    try:
        url = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi"
        params = {
            "db": "pubmed",
            "term": query,
            "retmode": "json",
            "rettype": "count"
        }
        response = requests.get(url, params=params, timeout=5)
        data = response.json()
        count = int(data["esearchresult"]["count"])
        return count
    except Exception:
        return None

# [New] ë²ˆì—­ í•¨ìˆ˜ ì¶”ê°€
@st.cache_data
def get_translated_title(text):
    try:
        # Google Translate GTX endpoint (Free, Unofficial)
        url = "https://translate.googleapis.com/translate_a/single"
        params = {
            "client": "gtx",
            "sl": "auto",
            "tl": "ko",
            "dt": "t",
            "q": text
        }
        response = requests.get(url, params=params, timeout=2)
        if response.status_code == 200:
            return response.json()[0][0][0]
    except Exception:
        pass
    return "ë²ˆì—­ ì‹¤íŒ¨ (ì—°ê²° í™•ì¸ í•„ìš”)"

# [New] í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŒ… í•¨ìˆ˜
def highlight_text(text):
    # í‚¤ì›Œë“œë“¤ì„ ì •ê·œì‹ íŒ¨í„´ìœ¼ë¡œ ì»´íŒŒì¼ (ëŒ€ì†Œë¬¸ì ë¬´ì‹œ)
    pattern = re.compile('|'.join(map(re.escape, EVIDENCE_KEYWORDS)), re.IGNORECASE)
    
    def replace(match):
        # ë§¤ì¹­ëœ ë‹¨ì–´ì— ìŠ¤íƒ€ì¼ ì ìš© (ì—°í•œ ì´ˆë¡ìƒ‰ ë°°ê²½)
        return f"<span style='background-color: #d1fae5; color: #065f46; padding: 0 4px; border-radius: 4px; font-weight: bold;'>{match.group(0)}</span>"
    
    return pattern.sub(replace, text)

# [New] ì„œì§€ ì •ë³´ ë‚´ë³´ë‚´ê¸° í•¨ìˆ˜ë“¤
def convert_to_bibtex(inventory_list):
    bibtex_entries = []
    for paper in inventory_list:
        # Citation Key ìƒì„± (ì²« ì €ì ì„± + ì—°ë„)
        first_author = paper['authors'][0].split()[-1] if paper['authors'] else "Unknown"
        # ì˜ë¬¸/ìˆ«ìë§Œ ë‚¨ê¸°ê¸°
        safe_key = re.sub(r'\W+', '', f"{first_author}{paper['year']}")
        
        authors_formatted = " and ".join(paper['authors'])
        
        entry = f"""@article{{{safe_key},
  title = {{{paper['title']}}},
  author = {{{authors_formatted}}},
  journal = {{{paper['journal']}}},
  year = {{{paper['year']}}},
  doi = {{{paper['id']}}}
}}"""
        bibtex_entries.append(entry)
    return "\n\n".join(bibtex_entries)

def convert_to_csv(inventory_list):
    # CSV Header
    lines = ["DOI,Title,Authors,Journal,Year,Citations,MyScore"]
    for paper in inventory_list:
        # CSV Escape (ë”°ì˜´í‘œ ì²˜ë¦¬)
        safe_title = paper['title'].replace('"', '""')
        safe_authors = "; ".join(paper['authors']).replace('"', '""')
        safe_journal = paper['journal'].replace('"', '""')
        score = paper.get('final_score', paper.get('debiased_score', 0))
        
        line = f"\"{paper['id']}\",\"{safe_title}\",\"{safe_authors}\",\"{safe_journal}\",{paper['year']},{paper['citations']},{score}"
        lines.append(line)
    return "\n".join(lines)

def evaluate_paper(paper_data):
    current_year = get_current_year()
    year = paper_data.get('year', current_year - 5)
    age = current_year - year
    title_lower = paper_data['title'].lower()
    citation_count = paper_data.get('citations', 0)
    
    # 1. í‚¤ì›Œë“œ (Evidence) - ìƒìˆ˜ë¡œ ë³€ê²½
    has_evidence = any(k in title_lower for k in EVIDENCE_KEYWORDS)
    
    # 2. ì—°êµ¬íŒ€ ê·œëª¨ (Team)
    author_count = paper_data.get('author_count', 1)
    is_big_team = author_count >= 5

    # 3. ë°ì´í„° ì‹ ë¢°ë„ (Reliability)
    ref_count = paper_data.get('ref_count') 
    integrity_status = "valid"
    risk_reason = ""

    if ref_count is None:
        if citation_count < 5:
            integrity_status = "uncertain"
            risk_reason = "ë©”íƒ€ë°ì´í„° ëˆ„ë½"
    elif ref_count < 5:
        if citation_count < 5:
            integrity_status = "suspected"
            risk_reason = "ì°¸ê³ ë¬¸í—Œ ë¶€ì¡±"

    # Score Calculation
    score_breakdown = {
        "Base": 30,
        "Evidence": 0,
        "Team": 0,
        "Volume Penalty": 0,
        "Integrity Penalty": 0
    }

    # 1. Raw Score (ì¸ê¸°ë„ ì¤‘ì‹¬) -> Impact
    raw_score = min(99, int(5 + (math.log(citation_count + 1) * 15)))

    # 2. Debiased Score (ë‚´ì‹¤ ì¤‘ì‹¬) -> Potential
    debiased_base = 30
    if has_evidence: 
        debiased_base += 30 
        score_breakdown["Evidence"] = 30
    if is_big_team: 
        debiased_base += 10
        score_breakdown["Team"] = 10
    
    # ë¬¸í—ŒëŸ‰ í¸í–¥ ì œê±°
    volume_discount = min(25, int(math.log(citation_count + 1) * 4))
    
    # ìµœì‹  ì—°êµ¬ ë³´ì •
    if age <= 2: volume_discount = int(volume_discount * 0.1)
    elif age <= 5: volume_discount = int(volume_discount * 0.5)

    score_breakdown["Volume Penalty"] = -volume_discount
    debiased_score = debiased_base - volume_discount
    
    if integrity_status != "valid":
        penalty = debiased_score - 5
        debiased_score = 5
        score_breakdown["Integrity Penalty"] = -penalty
        risk_reason = risk_reason or "ë°ì´í„° ì‹ ë¢°ë„ ë‚®ìŒ"
    elif age > 10 and citation_count < 5:
        penalty = debiased_score - 5
        debiased_score = 5
        score_breakdown["Integrity Penalty"] = -penalty
        risk_reason = "ë„íƒœëœ ì—°êµ¬"

    debiased_score = max(5, min(95, debiased_score))

    # 3. Bias Penalty & Type
    bias_penalty = raw_score - debiased_score
    
    potential_type = "normal"
    if debiased_score > 70 and bias_penalty < 0:
        potential_type = "amazing" 
    elif bias_penalty > 30:
        potential_type = "bubble" 
    elif integrity_status != "valid":
        potential_type = "bad"

    ai_score = debiased_score

    return {
        "raw_score": raw_score,
        "debiased_score": debiased_score,
        "bias_penalty": bias_penalty,
        "ai_score": ai_score,
        "potential_type": potential_type,
        "risk_reason": risk_reason,
        "has_evidence": has_evidence,
        "is_big_team": is_big_team,
        "integrity_status": integrity_status,
        "score_breakdown": score_breakdown,
        "age": age,
        "citation_count": citation_count
    }

def search_crossref_api(query):
    is_exact_mode = query.startswith('"') and query.endswith('"')
    clean_query = query.strip('"') if is_exact_mode else query
    
    try:
        url = f"https://api.crossref.org/works?query={clean_query}&rows=1000&sort=relevance"
        response = requests.get(url, timeout=20)
        data = response.json()
    except Exception as e:
        st.error(f"API ì—°ê²° ì˜¤ë¥˜: {e}")
        return [], {}, False

    if not data or not isinstance(data, dict): return [], {}, False
    message = data.get('message')
    if not message or not isinstance(message, dict): return [], {}, False
    items = message.get('items')
    if not items: return [], {}, False

    valid_papers = []
    current_year = get_current_year()

    pubmed_count = get_pubmed_count(clean_query)
    
    citations_list = []
    years_list = []

    for idx, item in enumerate(items):
        if not item.get('DOI'): continue
        if not item.get('title'): continue
        
        title_str = item['title'][0].lower()
        invalid_titles = ["announcement", "editorial", "issue info", "table of contents", "front matter", "back matter", "author index", "subject index", "correction", "erratum", "publisher's note", "conference info", "trial number", "trial registration", "clinicaltrials.gov", "identifier", "&na;", "unknown", "calendar", "masthead", "abstracts", "session", "meeting", "symposium", "workshop", "chinese journal", "test", "protocol", "data descriptor", "dataset"]
        if any(inv in title_str for inv in invalid_titles): continue
        
        cit = item.get('is-referenced-by-count', 0)
        citations_list.append(cit)
        
        y = None
        if item.get('published') and item['published'].get('date-parts'): y = item['published']['date-parts'][0][0]
        elif item.get('created') and item['created'].get('date-parts'): y = item['created']['date-parts'][0][0]
        if y: years_list.append(y)

        if not item.get('author'): continue
        authors_raw = item['author']
        valid_authors = []
        for a in authors_raw:
            given = a.get('given', '').strip()
            family = a.get('family', '').strip()
            full = f"{given} {family}".strip()
            if full and "&na;" not in full.lower() and "anonymous" not in full.lower():
                valid_authors.append(full)
        if not valid_authors: continue

        journal = item.get('container-title', ["Unknown Journal"])[0]
        ref_count = item.get('reference-count')
        pub_year = y if y else current_year - 5
        
        paper_data_for_eval = {
            'title': item['title'][0], 'year': pub_year, 'citations': cit, 
            'journal': journal, 'author_count': len(valid_authors), 'ref_count': ref_count
        }
        eval_result = evaluate_paper(paper_data_for_eval)

        paper_obj = {
            'id': item['DOI'],
            'title': item['title'][0],
            'authors': valid_authors[:3], 
            'author_full_count': len(valid_authors),
            'journal': journal,
            'year': pub_year,
            'citations': cit,
            'ref_count': ref_count if ref_count is not None else 0,
            'url': f"https://doi.org/{item['DOI']}",
            **eval_result,
            'is_reviewed': False,
            'original_rank': idx
        }
        valid_papers.append(paper_obj)
    
    avg_citations = int(sum(citations_list) / len(citations_list)) if citations_list else 0
    if years_list:
        year_counts = Counter(years_list)
        most_common_year = year_counts.most_common(1)[0][0]
        min_y, max_y = min(years_list), max(years_list)
        if max_y - min_y > 10: period_str = f"{most_common_year-2}~{most_common_year+2}"
        else: period_str = f"{min_y}~{max_y}"
    else:
        period_str = "Unknown"

    bias_summary = {
        "pubmed_count": pubmed_count if pubmed_count is not None else "ì§‘ê³„ ë¶ˆê°€",
        "avg_citations": avg_citations,
        "period": period_str,
        "is_high_exposure": (pubmed_count > 5000 if pubmed_count else False) or avg_citations > 100
    }

    if not is_exact_mode:
        valid_papers.sort(key=lambda x: x['debiased_score'], reverse=True)
            
    return valid_papers, bias_summary, is_exact_mode

# --- 3. Streamlit UI ---

st.set_page_config(page_title="Research Simulator", page_icon="ğŸ“", layout="wide")

if 'user_id' not in st.session_state: st.session_state['user_id'] = None
if 'score' not in st.session_state: st.session_state['score'] = 0
if 'inventory' not in st.session_state: st.session_state['inventory'] = []
if 'trash' not in st.session_state: st.session_state['trash'] = []
if 'mission_id' not in st.session_state: st.session_state['mission_id'] = 1
if 'search_results' not in st.session_state: st.session_state['search_results'] = []
if 'bias_summary' not in st.session_state: st.session_state['bias_summary'] = {}
if 'search_page' not in st.session_state: st.session_state['search_page'] = 1
if 'analysis_page' not in st.session_state: st.session_state['analysis_page'] = 1
if 'is_exact_search' not in st.session_state: st.session_state['is_exact_search'] = False
if 'sort_option' not in st.session_state: st.session_state['sort_option'] = "Potential"
if 'analysis_weights' not in st.session_state: st.session_state['analysis_weights'] = {"evidence": 1.0, "recency": 1.0, "team": 1.0, "scarcity": 1.0}
if 'current_preset' not in st.session_state: st.session_state['current_preset'] = "âš–ï¸ ë°¸ëŸ°ìŠ¤"

def get_level_info(score):
    level_threshold = 500
    level = (score // level_threshold) + 1
    progress = (score % level_threshold) / level_threshold
    next_milestone = (level) * level_threshold
    return level, progress, next_milestone

def check_mission(paper, action):
    current_m = next((m for m in MISSIONS if m['id'] == st.session_state.mission_id), None)
    if not current_m: return
    completed = False
    m_type = current_m['type']
    if m_type == "citation" and action == "collect" and paper['citations'] >= 100: completed = True
    elif m_type == "team" and action == "collect" and paper['is_big_team']: completed = True
    elif m_type == "score" and st.session_state.score >= current_m['target']: completed = True
    if completed:
        st.session_state.score += current_m['reward']
        st.session_state.mission_id += 1
        st.toast(f"ğŸ‰ ë¯¸ì…˜ ì™„ë£Œ! ë³´ìƒ +{current_m['reward']}ì ", icon="ğŸ")
        if st.session_state.get("user_id"): save_user_data(st.session_state.user_id)

# Login Screen
if not st.session_state.get("user_id"):
    st.title("ğŸ“ AI ê¸°ë°˜ ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œ")
    st.caption("ìº¡ìŠ¤í†¤ ë””ìì¸ _ AI:D")
    st.markdown("---")
    st.markdown("### ğŸ‘‹ í™˜ì˜í•©ë‹ˆë‹¤!")
    st.info("ì—°êµ¬ì IDë¥¼ ì…ë ¥í•˜ì—¬ ê²€ìƒ‰ì„ ì‹œì‘í•˜ì„¸ìš”.")
    col1, col2 = st.columns([3, 1])
    with col1: user_input = st.text_input("ì—°êµ¬ì ì´ë¦„ (ID)", placeholder="ì˜ˆ: Dr.Kim")
    with col2:
        st.write(""); st.write("")
        if st.button("ë¡œê·¸ì¸ / ì‹œì‘", type="primary", use_container_width=True):
            if user_input:
                st.session_state.user_id = user_input
                saved_data = load_user_data(user_input)
                st.session_state.score = saved_data["score"]
                st.session_state.inventory = saved_data["inventory"]
                st.session_state.mission_id = saved_data["mission_id"]
                st.session_state.trash = saved_data["trash"]
                st.rerun()
            else: st.warning("ì´ë¦„ì„ ì…ë ¥í•´ì£¼ì„¸ìš”.")
    st.stop() 

# Sidebar
with st.sidebar:
    st.title("ğŸ“ AI ê¸°ë°˜ ë…¼ë¬¸ ì¶”ì²œ ì‹œìŠ¤í…œ")
    st.caption("ìº¡ìŠ¤í†¤ ë””ìì¸ _ AI:D")
    st.info(f"ğŸ‘¤ {st.session_state.user_id} ì—°êµ¬ì›")
    if st.button("ë¡œê·¸ì•„ì›ƒ (ì €ì¥ë¨)", use_container_width=True):
        save_user_data(st.session_state.user_id)
        st.session_state.user_id = None
        st.rerun()
    st.divider()
    current_level, progress, next_score = get_level_info(st.session_state.score)
    st.metric("ì—°êµ¬ ë ˆë²¨", f"Lv. {current_level}")
    st.write(f"í˜„ì¬ ì ìˆ˜: {st.session_state.score} / {next_score}")
    st.progress(progress)
    st.metric("ë³´ìœ  ë…¼ë¬¸", f"{len(st.session_state.inventory)}í¸")
    st.divider()
    
    st.markdown("#### ğŸ” í‰ê°€ ì§€í‘œ ê°€ì´ë“œ")
    st.markdown("""
    **1. Impact (ì˜í–¥ë ¥)**
    : ê¸°ì¡´ì˜ ì¸ê¸°ë„ ì ìˆ˜(Raw Score). ì¸ìš©ìˆ˜ì™€ ì €ë„ ì¸ì§€ë„ ë“± í•™ê³„ì—ì„œì˜ í˜„ì¬ ìœ„ìƒì„ ë°˜ì˜í•©ë‹ˆë‹¤.
    
    **2. Potential (ì ì¬ë ¥)**
    : ì¸ìš© ê±°í’ˆì„ ì œê±°í•œ ë‚´ì‹¤ ì ìˆ˜(Debiased Score). ì¦ê±° ê¸°ë°˜ì˜ í¬ì†Œì„±ê³¼ ë¯¸ë˜ ê°€ì¹˜ë¥¼ í‰ê°€í•©ë‹ˆë‹¤.
    
    **3. Bias Penalty (í¸í–¥)**
    : Impactì™€ Potentialì˜ ê´´ë¦¬. ì–‘ìˆ˜ë©´ ê³¼ì—´(Bubble), ìŒìˆ˜ë©´ ì €í‰ê°€(Hidden Gem)ëœ ì—°êµ¬ì…ë‹ˆë‹¤.
    """)

    st.markdown("#### ğŸ“Š ì ìˆ˜ ìƒì„¸ ì§€í‘œ")
    st.markdown("""
    **1. Evidence (ì¦ê±°)**
    - **ë°©ì‹**: ì œëª© ë‚´ ì‹¤í—˜ í‚¤ì›Œë“œ(in vivo, clinical ë“±) í¬í•¨ ì—¬ë¶€
    - **ì ìˆ˜**: í¬í•¨ ì‹œ 30ì  (ë¯¸í¬í•¨ 0ì )
    - **ì˜ë¯¸**: ì‹¤ì§ˆì  ì‹¤í—˜ ë°ì´í„°ê°€ ìˆëŠ” ë…¼ë¬¸ ìš°ëŒ€

    **2. Recency (ìµœì‹ ì„±)**
    - **ë°©ì‹**: (5 - ê²½ê³¼ë…„ìˆ˜) * 10 (ìµœëŒ€ 50ì )
    - **ì ìˆ˜**: ìµœì‹ ìˆœ 50ì  ~ 5ë…„ ì´ìƒ 0ì 
    - **ì˜ë¯¸**: ìµœì‹  ì—°êµ¬ì¼ìˆ˜ë¡ ê³ ë“ì 

    **3. Scarcity (í¬ì†Œì„±)**
    - **ë°©ì‹**: 50 - ì¸ìš© íšŸìˆ˜ (ìµœì†Œ 0ì )
    - **ì ìˆ˜**: ì¸ìš© 0íšŒ ì‹œ 50ì  ~ 50íšŒ ì´ìƒ 0ì 
    - **ì˜ë¯¸**: ì¸ìš©ì´ ì ì€ ìˆ¨ê²¨ì§„ ë…¼ë¬¸(Hidden Gem) ë°œêµ´

    **4. Team (ê·œëª¨)**
    - **ë°©ì‹**: ì €ì 5ëª… ì´ìƒ ì—¬ë¶€
    - **ì ìˆ˜**: 5ëª… ì´ìƒ ì‹œ 10ì  (ë¯¸ë§Œ 0ì )
    - **ì˜ë¯¸**: ëŒ€ê·œëª¨ í˜‘ì—… ì—°êµ¬ ë°˜ì˜
    """)

    st.markdown("#### ğŸ“Š ê²€ìƒ‰ ë°©ë²•")
    st.markdown("""
    1. ì¼ë°˜ ê²€ìƒ‰
        : AI ì¶”ì²œ ì§€ìˆ˜(Potential)ê°€ ë†’ì€ ìˆœìœ¼ë¡œ ì¶”ì²œ
    2. "í‚¤ì›Œë“œ"
        : ë”°ì˜´í‘œ ê²€ìƒ‰ ì‹œ ì •í™•ë„ ìˆœìœ¼ë¡œ ê²°ê³¼ ë…¸ì¶œ
    """)
    
    st.divider()
    # [New] Mobile Support Option
    show_translation = st.checkbox("í•œê¸€ ë²ˆì—­ í•­ìƒ ë³´ê¸° (ëª¨ë°”ì¼ìš©)", value=False)
    # [New] Highlight Option
    show_highlight = st.checkbox("í‚¤ì›Œë“œ í•˜ì´ë¼ì´íŒ… (Visual Evidence)", value=True, help="ì ìˆ˜ì— ê¸ì •ì  ì˜í–¥ì„ ì¤€ í•µì‹¬ ë‹¨ì–´ë¥¼ ê°•ì¡°í•©ë‹ˆë‹¤.")

tab_search, tab_analysis, tab_inventory, tab_trash = st.tabs(["ğŸ” ë…¼ë¬¸ ê²€ìƒ‰", "ğŸ“Š ì§€í‘œ ë¶„ì„", "ğŸ“š ë‚´ ì„œì¬", "ğŸ—‘ï¸ íœ´ì§€í†µ"])

# --- íƒ­ 1: ë…¼ë¬¸ ê²€ìƒ‰ ---
with tab_search:
    col1, col2 = st.columns([4, 1])
    with col1: query = st.text_input("í‚¤ì›Œë“œ ì…ë ¥", placeholder='ì˜ˆ: "Immunotherapy" (ë”°ì˜´í‘œëŠ” ì •í™•ë„ìˆœ)')
    with col2:
        st.write(""); st.write("")
        search_btn = st.button("ê²€ìƒ‰", type="primary", use_container_width=True)

    if search_btn and query:
        with st.spinner("ë¬¸í—ŒëŸ‰ í¸í–¥ ë¶„ì„ ë° ë°ì´í„° ì²˜ë¦¬ ì¤‘..."):
            results, summary, is_exact = search_crossref_api(query)
            st.session_state.search_results = results
            st.session_state.bias_summary = summary
            st.session_state.is_exact_search = is_exact
            st.session_state.search_page = 1 
            st.session_state.sort_option = "ì •í™•ë„" if is_exact else "Potential"
            if not results: st.error("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

    if st.session_state.search_results:
        summary = st.session_state.bias_summary
        with st.container(border=True):
            st.markdown("""<div style="font-size: 1.5rem; font-weight: 600; margin-bottom: 1rem;">ğŸ” í¸í–¥ ìš”ì•½</div>""", unsafe_allow_html=True)
            bc1, bc2, bc3 = st.columns(3)
            pub_cnt = summary['pubmed_count']
            pub_cnt_str = f"{pub_cnt:,}í¸" if isinstance(pub_cnt, int) else str(pub_cnt)
            with bc1: st.metric("PubMed ë…¼ë¬¸ ìˆ˜", pub_cnt_str)
            with bc2: st.metric("í‰ê·  ì¸ìš©ìˆ˜ (Top 200)", f"{summary['avg_citations']:,}íšŒ")
            with bc3: st.metric("ì—°êµ¬ ì§‘ì¤‘ ì‹œê¸°", summary['period'])
            if summary['is_high_exposure']:
                st.warning("âš  **High Exposure Topic**: ì—°êµ¬ê°€ ë§¤ìš° í™œë°œí•˜ì—¬, ìƒìœ„ ë…¸ì¶œ ë…¼ë¬¸ì˜ Impact(ì˜í–¥ë ¥)ê°€ ê³¼ëŒ€í‰ê°€ë˜ì—ˆì„ ê°€ëŠ¥ì„±ì´ í½ë‹ˆë‹¤. Potential(ì ì¬ë ¥) ì§€í‘œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.")
            else:
                st.success("âœ… **Niche Topic**: ë¹„êµì  ì—°êµ¬ê°€ ëœ ëœ ë¶„ì•¼ì…ë‹ˆë‹¤. ìˆ¨ê²¨ì§„ ëª…ì‘ì´ ë§ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        st.divider()

        st.markdown("""<div style="font-size: 1rem; font-weight: 600; margin-bottom: 1rem;">ğŸ”ƒ ì •ë ¬ ê¸°ì¤€ ì„ íƒ</div>""", unsafe_allow_html=True)
        sort_col, _ = st.columns([2, 1])
        with sort_col:
            sort_opt = st.radio(
                "ì •ë ¬ ê¸°ì¤€", 
                ["Potential (ì ì¬ë ¥)", "Impact (ì˜í–¥ë ¥)", "ìµœì‹ ", "ì •í™•ë„"], 
                horizontal=True, 
                label_visibility="collapsed", 
                key="sort_selector"
            )
        
        if "Potential" in sort_opt:
            st.session_state.search_results.sort(key=lambda x: x['debiased_score'], reverse=True)
        elif "Impact" in sort_opt:
            st.session_state.search_results.sort(key=lambda x: x['raw_score'], reverse=True)
        elif sort_opt == "ìµœì‹ ":
            st.session_state.search_results.sort(key=lambda x: x['year'], reverse=True)
        elif sort_opt == "ì •í™•ë„":
            st.session_state.search_results.sort(key=lambda x: x['original_rank'])

        items_per_page = 50
        total_items = len(st.session_state.search_results)
        total_pages = max(1, math.ceil(total_items / items_per_page))
        current_page = st.session_state.search_page
        start_idx = (current_page - 1) * items_per_page
        end_idx = start_idx + items_per_page
        page_items = st.session_state.search_results[start_idx:end_idx]

        st.caption(f"ê²€ìƒ‰ ê²°ê³¼ ì´ {total_items}ê±´ | ì •ë ¬: {sort_opt} | í˜ì´ì§€: {current_page}/{total_pages}")
        
        for i, paper in enumerate(page_items):
            unique_key_idx = start_idx + i
            with st.container(border=True):
                c1, c2 = st.columns([5, 2])
                with c1:
                    # [Changed] Title Display with Tooltip & Translation & Highlight
                    translated_title = get_translated_title(paper['title'])
                    display_title = highlight_text(paper['title']) if show_highlight else paper['title']
                    
                    st.markdown(
                        f"""<div title="[ë²ˆì—­] {translated_title}" style="font-size:1.2rem; font-weight:bold; margin-bottom:5px;">{display_title}</div>""", 
                        unsafe_allow_html=True
                    )
                    if show_translation:
                        st.caption(f"ğŸ‡°ğŸ‡· {translated_title}")
                    
                    tags = []
                    if paper['has_evidence']: tags.append("ğŸ”¬ Evidence")
                    if paper['is_big_team']: tags.append("ğŸ‘¥ Big Team")
                    if paper['integrity_status'] != "valid": tags.append("âš ï¸ ë°ì´í„° ë¶€ì¡±")
                    if paper['potential_type'] == "amazing": tags.append("ğŸ’ Hidden Gem")
                    st.write(" ".join([f"`{t}`" for t in tags]))
                    auth_display = ", ".join(paper['authors'])
                    if paper['author_full_count'] > 3: auth_display += f" ì™¸ {paper['author_full_count'] - 3}ëª…"
                    st.caption(f"{paper['year']} | {paper['journal']} | ì¸ìš© {paper['citations']}íšŒ | ì €ì: {auth_display}")
                    
                    st.markdown(f"[ğŸ“„ ì›ë¬¸ ë³´ê¸°]({paper['url']})")

                with c2:
                    col_raw, col_deb = st.columns(2)
                    with col_raw: st.metric("Impact", f"{paper['raw_score']}", help="í˜„ì¬ í•™ê³„ì—ì„œì˜ ì˜í–¥ë ¥ ë° ì¸ê¸°ë„ (Raw Score)")
                    with col_deb: st.metric("Potential", f"{paper['debiased_score']}", delta=f"{-paper['bias_penalty']}", help="ë¯¸ë˜ ê°€ì¹˜ ë° ì ì¬ë ¥ (Debiased Score)")
                    if paper['bias_penalty'] > 20: st.caption("âš  ê³¼ì—´ë¨")

                    is_owned = any(p['id'] == paper['id'] for p in st.session_state.inventory)
                    if is_owned:
                        st.button("ë³´ìœ ì¤‘", key=f"owned_{unique_key_idx}", disabled=True, use_container_width=True)
                    else:
                        if st.button("ìˆ˜ì§‘", key=f"collect_{unique_key_idx}", type="secondary", use_container_width=True):
                            st.session_state.inventory.append(paper)
                            st.session_state.score += paper['debiased_score']
                            check_mission(paper, "collect")
                            save_user_data(st.session_state.user_id) 
                            st.rerun()
        
        st.divider()
        _, nav_col, _ = st.columns([1, 5, 1])
        with nav_col:
            if total_pages <= 5: display_pages = range(1, total_pages + 1)
            else:
                if current_page <= 3: display_pages = range(1, 6)
                elif current_page >= total_pages - 2: display_pages = range(total_pages - 4, total_pages + 1)
                else: display_pages = range(current_page - 2, current_page + 3)

            pg_cols = st.columns([1, 1, 1, 1, 1, 1, 1, 0.5, 2.5], gap="small")
            with pg_cols[0]:
                if st.button("â—€", key="nav_prev", disabled=current_page==1, use_container_width=True):
                    st.session_state.search_page -= 1
                    st.rerun()
            
            for idx, p_num in enumerate(display_pages):
                if idx < 5:
                    with pg_cols[idx + 1]:
                        b_type = "primary" if p_num == current_page else "secondary"
                        if st.button(f"{p_num}", key=f"nav_p_{p_num}", type=b_type, use_container_width=True):
                            st.session_state.search_page = p_num
                            st.rerun()
            
            with pg_cols[6]:
                if st.button("â–¶", key="nav_next", disabled=current_page==total_pages, use_container_width=True):
                    st.session_state.search_page += 1
                    st.rerun()
            with pg_cols[8]:
                 new_page = st.number_input("ì´ë™", min_value=1, max_value=total_pages, value=current_page, label_visibility="collapsed", key="nav_an_input")
                 if new_page != current_page:
                    st.session_state.search_page = new_page
                    st.rerun()

# --- [New] Analysis Tab (í˜ì´ì§€ë„¤ì´ì…˜ ì ìš©) ---
with tab_analysis:
    if not st.session_state.search_results:
        st.info("ë¨¼ì € 'ë…¼ë¬¸ ê²€ìƒ‰' íƒ­ì—ì„œ ê²€ìƒ‰ì„ ìˆ˜í–‰í•´ì£¼ì„¸ìš”.")
    else:
        st.markdown("""<div style="font-size: 1.5rem; font-weight: 600; margin-bottom: 1rem;">ğŸ› ï¸ ë§ì¶¤í˜• ì§€í‘œ ë¶„ì„</div>""", unsafe_allow_html=True)
        st.markdown("ê° ì§€í‘œì˜ ê°€ì¤‘ì¹˜ë¥¼ ì¡°ì ˆí•˜ì—¬ ë‚˜ë§Œì˜ ê¸°ì¤€(Custom Potential)ìœ¼ë¡œ ë…¼ë¬¸ì„ ì¬í‰ê°€í•˜ê³  ì •ë ¬í•©ë‹ˆë‹¤.")
        
        if 'analysis_weights' not in st.session_state:
            st.session_state.analysis_weights = {"evidence": 1.0, "recency": 1.0, "team": 1.0, "scarcity": 1.0}
            st.session_state.current_preset = "âš–ï¸ ë°¸ëŸ°ìŠ¤"
        
        # [Fix] Initialize analysis page
        if 'analysis_page' not in st.session_state:
            st.session_state.analysis_page = 1

        col_p1, col_p2, col_p3, col_p4 = st.columns(4)
        
        with col_p1:
            if st.button("âš–ï¸ ë°¸ëŸ°ìŠ¤", use_container_width=True, help="ëª¨ë“  ì§€í‘œë¥¼ ê³¨ê³ ë£¨ ë°˜ì˜í•©ë‹ˆë‹¤."):
                st.session_state.analysis_weights = {"evidence": 1.0, "recency": 1.0, "team": 1.0, "scarcity": 1.0}
                st.session_state.current_preset = "âš–ï¸ ë°¸ëŸ°ìŠ¤"
                st.rerun()

        with col_p2:
            if st.button("ğŸ’ ìˆ¨ê²¨ì§„ ì›ì„", use_container_width=True, help="ì¸ìš©ì€ ì ì§€ë§Œ ì¦ê±°ê°€ í™•ì‹¤í•œ ë…¼ë¬¸ì„ ì°¾ìŠµë‹ˆë‹¤."):
                st.session_state.analysis_weights = {"evidence": 2.0, "recency": 1.0, "team": 1.0, "scarcity": 3.0}
                st.session_state.current_preset = "ğŸ’ ìˆ¨ê²¨ì§„ ì›ì„"
                st.rerun()
                
        with col_p3:
            if st.button("ğŸš€ ìµœì‹  íŠ¸ë Œë“œ", use_container_width=True, help="ìµœì‹ ì„±ê³¼ ì‹¤í—˜ì  ê·¼ê±°ë¥¼ ìµœìš°ì„ ìœ¼ë¡œ ë´…ë‹ˆë‹¤."):
                st.session_state.analysis_weights = {"evidence": 2.0, "recency": 3.0, "team": 0.5, "scarcity": 1.0}
                st.session_state.current_preset = "ğŸš€ ìµœì‹  íŠ¸ë Œë“œ"
                st.rerun()

        with col_p4:
            if st.button("ğŸ‘‘ ëŒ€ê·œëª¨", use_container_width=True, help="ëŒ€ê·œëª¨ ì—°êµ¬íŒ€ì„ ì„ í˜¸í•©ë‹ˆë‹¤."):
                st.session_state.analysis_weights = {"evidence": 1.0, "recency": 0.5, "team": 3.0, "scarcity": 0.5}
                st.session_state.current_preset = "ğŸ‘‘ ëŒ€ê·œëª¨"
                st.rerun()

        st.info(f"í˜„ì¬ ì ìš©ëœ ë¶„ì„ ëª¨ë“œ: **{st.session_state.current_preset}**")

        # --- [ì¶”ê°€ëœ ë¶€ë¶„] ê°€ì¤‘ì¹˜ ì„¤ëª… ë¬¸êµ¬ ---
        st.markdown("""
        <small>ğŸ’¡ **ê°€ì¤‘ì¹˜ ì„¤ì • ê°€ì´ë“œ**: ìŠ¬ë¼ì´ë”ì˜ ìˆ«ìëŠ” í•´ë‹¹ ì§€í‘œì˜ ì¤‘ìš”ë„(ë°°ìˆ˜)ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.
        <br>â€¢ **1.0**: ê¸°ë³¸ ë°˜ì˜ | â€¢ **2.0**: 2ë°° ë” ì¤‘ìš”í•˜ê²Œ ë°˜ì˜ | â€¢ **0.0**: ì ìˆ˜ ì‚°ì •ì—ì„œ ì œì™¸</small>
        """, unsafe_allow_html=True)
        # ------------------------------------

        w = st.session_state.analysis_weights
        
        # [ìˆ˜ì •] ìŠ¬ë¼ì´ë”: í•œêµ­ì–´, ì°¨íŠ¸: ì˜ì–´(í•œêµ­ì–´)
        with st.container(border=True):
            col_w1, col_w2 = st.columns(2)
            with col_w1: w["evidence"] = st.slider("ì¦ê±°", 0.0, 3.0, w["evidence"])
            with col_w2: w["recency"] = st.slider("ìµœì‹ ì„±", 0.0, 3.0, w["recency"])
            col_w3, col_w4 = st.columns(2)
            with col_w3: w["team"] = st.slider("ê·œëª¨", 0.0, 3.0, w["team"])
            with col_w4: w["scarcity"] = st.slider("í¬ì†Œì„±", 0.0, 3.0, w["scarcity"])

        w_evidence = w["evidence"]
        w_recency = w["recency"]
        w_team = w["team"]
        w_scarcity = w["scarcity"]

        analyzed_papers = []
        for paper in st.session_state.search_results:
            details = paper.get('score_breakdown', {})
            # Base ì œê±°ë¨
            ev_score = details.get('Evidence', 0)
            team_score = details.get('Team', 0)
            vol_penalty = details.get('Volume Penalty', 0)
            age_score = max(0, (5 - paper.get('age', 5)) * 10)
            scarcity_score = max(0, (50 - paper.get('citation_count', 0))) 
            if scarcity_score > 50: scarcity_score = 50
            
            custom_score = (
                (ev_score * w_evidence) +
                (team_score * w_team) +
                (age_score * w_recency) +
                (scarcity_score * w_scarcity) +
                vol_penalty
            )
            paper_copy = paper.copy()
            paper_copy['custom_score'] = int(custom_score)
            analyzed_papers.append(paper_copy)
            
        analyzed_papers.sort(key=lambda x: x['custom_score'], reverse=True)
        
        # [New] Analysis Tab Pagination
        items_per_page = 50
        total_items_an = len(analyzed_papers)
        total_pages_an = max(1, math.ceil(total_items_an / items_per_page))
        current_page_an = st.session_state.analysis_page
        
        start_idx_an = (current_page_an - 1) * items_per_page
        end_idx_an = start_idx_an + items_per_page
        page_items_an = analyzed_papers[start_idx_an:end_idx_an]

        st.divider()
        st.caption(f"ì¬í‰ê°€ ê²°ê³¼ ({total_items_an}ê±´) | í˜ì´ì§€: {current_page_an}/{total_pages_an}")
        
        for i, paper in enumerate(page_items_an):
            # Key collision prevention
            unique_an_key = f"an_{start_idx_an + i}"
            with st.container(border=True):
                c1, c2 = st.columns([5, 2])
                with c1:
                    # [Changed] Title Display with Tooltip & Translation & Highlight
                    translated_title = get_translated_title(paper['title'])
                    display_title = highlight_text(paper['title']) if show_highlight else paper['title']
                    
                    st.markdown(
                        f"""<div title="[ë²ˆì—­] {translated_title}" style="font-size:1.1rem; font-weight:bold; margin-bottom:5px;">{start_idx_an + i + 1}. {display_title}</div>""", 
                        unsafe_allow_html=True
                    )
                    if show_translation:
                        st.caption(f"ğŸ‡°ğŸ‡· {translated_title}")
                    
                    # [New] ê¸°ë³¸ ì •ë³´ í‘œì‹œ ì¶”ê°€
                    tags = []
                    if paper['has_evidence']: tags.append("ğŸ”¬ Evidence")
                    if paper['is_big_team']: tags.append("ğŸ‘¥ Big Team")
                    if paper['integrity_status'] != "valid": tags.append("âš ï¸ ë°ì´í„° ë¶€ì¡±")
                    if paper['potential_type'] == "amazing": tags.append("ğŸ’ Hidden Gem")
                    st.write(" ".join([f"`{t}`" for t in tags]))
                    
                    auth_display = ", ".join(paper['authors'])
                    if paper['author_full_count'] > 3: auth_display += f" ì™¸ {paper['author_full_count'] - 3}ëª…"
                    st.caption(f"{paper['year']} | {paper['journal']} | ì¸ìš© {paper['citations']}íšŒ | ì €ì: {auth_display}")
                    st.markdown(f"[ğŸ“„ ì›ë¬¸ ë³´ê¸°]({paper['url']})")

                    with st.expander("ì ìˆ˜ ìƒì„¸ êµ¬ì„± ë³´ê¸°"):
                        details = paper.get('score_breakdown', {})
                        # [Modified] Chart Keys: English (Korean)
                        chart_data = {
                            "Evidence (ì¦ê±°)": details.get('Evidence', 0) * w_evidence,
                            "Team (ê·œëª¨)": details.get('Team', 0) * w_team,
                            "Recency (ìµœì‹ ì„±)": max(0, (5 - paper.get('age', 5)) * 10) * w_recency,
                            "Scarcity (í¬ì†Œì„±)": max(0, (50 - paper.get('citation_count', 0))) * w_scarcity,
                        }
                        st.bar_chart(chart_data, horizontal=True)
                with c2:
                    st.metric("ì‚¬ìš©ì ì ìˆ˜", f"{paper['custom_score']}")
                    is_owned = any(p['id'] == paper['id'] for p in st.session_state.inventory)
                    if is_owned:
                        st.button("ë³´ìœ ì¤‘", key=f"an_own_{unique_an_key}", disabled=True, use_container_width=True)
                    else:
                        if st.button("ìˆ˜ì§‘", key=f"an_col_{unique_an_key}", type="secondary", use_container_width=True):
                            st.session_state.inventory.append(paper)
                            st.session_state.score += paper['debiased_score']
                            check_mission(paper, "collect")
                            save_user_data(st.session_state.user_id)
                            st.rerun()

        # [New] Analysis Pagination Controller
        st.divider()
        _, nav_col_an, _ = st.columns([1, 5, 1])
        with nav_col_an:
            if total_pages_an <= 5: display_pages_an = range(1, total_pages_an + 1)
            else:
                if current_page_an <= 3: display_pages_an = range(1, 6)
                elif current_page_an >= total_pages_an - 2: display_pages_an = range(total_pages_an - 4, total_pages_an + 1)
                else: display_pages_an = range(current_page_an - 2, current_page_an + 3)

            pg_cols_an = st.columns([1, 1, 1, 1, 1, 1, 1, 0.5, 2.5], gap="small")
            
            with pg_cols_an[0]:
                if st.button("â—€", key="nav_an_prev", disabled=current_page_an==1, use_container_width=True):
                    st.session_state.analysis_page -= 1
                    st.rerun()
            
            for idx, p_num in enumerate(display_pages_an):
                if idx < 5:
                    with pg_cols_an[idx + 1]:
                        b_type = "primary" if p_num == current_page_an else "secondary"
                        if st.button(f"{p_num}", key=f"nav_an_p_{p_num}", type=b_type, use_container_width=True):
                            st.session_state.analysis_page = p_num
                            st.rerun()
            
            with pg_cols_an[6]:
                if st.button("â–¶", key="nav_an_next", disabled=current_page_an==total_pages_an, use_container_width=True):
                    st.session_state.analysis_page += 1
                    st.rerun()
            with pg_cols_an[8]:
                 new_page_an = st.number_input("ì´ë™", min_value=1, max_value=total_pages_an, value=current_page_an, label_visibility="collapsed", key="nav_an_input")
                 if new_page_an != current_page_an:
                    st.session_state.analysis_page = new_page_an
                    st.rerun()

with tab_inventory:
    # [New] Layout: Main List (Left) | Value Guide (Right)
    inv_main, inv_info = st.columns([3, 1])
    
    with inv_info:
        with st.container(border=True):
            st.markdown("""<div style="font-size: 1.2rem; font-weight: 600; margin-bottom: 1rem;">ğŸ’¡ ê°€ì¹˜ ì‚°ì • ê³µì‹</div>""", unsafe_allow_html=True)
            st.markdown("""
            **1. ì‹¬ì¸µ ê²€ì¦ (ì„±ê³µ)**
            > **Potential + 50% ë³´ë„ˆìŠ¤**
            
            <small>ì¢‹ì€ ì›ì„(Potential)ì„ ë°œêµ´í• ìˆ˜ë¡, ì—°êµ¬ìì˜ ê²€ì¦ì„ í†µí•´ ê·¸ ê°€ì¹˜ê°€ 1.5ë°°ë¡œ ì¦í­ë©ë‹ˆë‹¤.</small>
            """, unsafe_allow_html=True)
            
            st.markdown("---")
            
            st.markdown("""
            **2. ê°•ì œ ìŠ¹ì¸ (ë¦¬ìŠ¤í¬)**
            > **Potential + 10ì **
            
            <small>ë°ì´í„°ê°€ ë¶€ì¡±í•œ(Risk) ë…¼ë¬¸ì„ ì–µì§€ë¡œ ìŠ¹ì¸í•˜ë©´, ë³´ë„ˆìŠ¤ê°€ ëŒ€í­ ì¶•ì†Œë©ë‹ˆë‹¤.</small>
            """, unsafe_allow_html=True)

    with inv_main:
        if not st.session_state.inventory: 
            st.info("ìˆ˜ì§‘ëœ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")
        else:
            # [New] Export UI - Added based on user request
            with st.expander("ğŸ“‚ ì„œì§€ ì •ë³´ ë‚´ë³´ë‚´ê¸° (BibTeX / CSV)"):
                e_col1, e_col2 = st.columns(2)
                with e_col1:
                    bib_data = convert_to_bibtex(st.session_state.inventory)
                    st.download_button(
                        label="BibTeX ë‹¤ìš´ë¡œë“œ (.bib)",
                        data=bib_data,
                        file_name="my_research_inventory.bib",
                        mime="text/plain",
                        use_container_width=True
                    )
                with e_col2:
                    csv_data = convert_to_csv(st.session_state.inventory)
                    st.download_button(
                        label="CSV ë‹¤ìš´ë¡œë“œ (.csv)",
                        data=csv_data,
                        file_name="my_research_inventory.csv",
                        mime="text/csv",
                        use_container_width=True
                    )
            
            # [New] BibTeX Usage Guide (Separate Expander)
            with st.expander("ğŸ“– Overleafë¡œ BibTeX ì“°ëŠ” ì´ˆê°„ë‹¨ ë£¨íŠ¸ (ê°€ì´ë“œ ë³´ê¸°)"):
                st.markdown("ğŸ”— [Overleaf ë¡œê·¸ì¸ ë°”ë¡œê°€ê¸°](https://www.overleaf.com/login)")
                
                st.markdown(r"""
                BibTeXì—ì„œ .bib íŒŒì¼ì€ ì°¸ê³ ë¬¸í—Œì´ â€œì¶œë ¥ëœ ê²°ê³¼ë¬¼â€ì´ ì•„ë‹ˆë¼, ë…¼ë¬¸ ì •ë³´ê°€ ì •ë¦¬ë˜ì–´ ìˆëŠ” ë°ì´í„° íŒŒì¼ì— í•´ë‹¹í•œë‹¤. ê·¸ë˜ì„œ .bib íŒŒì¼ì„ ê·¸ëƒ¥ ì—´ì–´ì„œëŠ” ì°¸ê³ ë¬¸í—Œ ëª©ë¡ì´ ë³´ì´ì§€ ì•Šê³ , ë°˜ë“œì‹œ LaTeX ë¬¸ì„œê°€ ì´ íŒŒì¼ì„ ë¶ˆëŸ¬ì™€ PDFë¡œ ì¶œë ¥í•´ ì£¼ì–´ì•¼ í•œë‹¤. Overleafë¥¼ ì‚¬ìš©í•˜ëŠ” ì´ìœ ëŠ” ì´ ê³¼ì •ì„ ê°€ì¥ ê°„ë‹¨í•˜ê²Œ ì²˜ë¦¬í•´ ì£¼ê¸° ë•Œë¬¸ì´ë‹¤.
                
                Overleafì—ì„œëŠ” ë¨¼ì € ìƒˆ í”„ë¡œì íŠ¸ë¥¼ ë§Œë“¤ê³ , ê¸°ë³¸ìœ¼ë¡œ ìƒì„±ëœ main.tex íŒŒì¼ê³¼ í•¨ê»˜ ê°€ì§€ê³  ìˆëŠ” .bib íŒŒì¼ì„ ê°™ì€ í”„ë¡œì íŠ¸ ì•ˆì— ì—…ë¡œë“œí•œë‹¤. ê·¸ë‹¤ìŒ main.texì—ì„œ ë³¸ë¬¸ì„ ì‘ì„±í•˜ê³ , ë¬¸ì„œì˜ ëë¶€ë¶„, ì¦‰ `\end{document}` ë°”ë¡œ ìœ„ì— BibTeX ê´€ë ¨ ì½”ë“œë¥¼ ì¶”ê°€í•œë‹¤. ì´ë•Œ `\bibliography{references}`ëŠ” â€œreferences.bibë¼ëŠ” íŒŒì¼ì„ ì°¸ê³ ë¬¸í—Œ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ê² ë‹¤â€ëŠ” ì˜ë¯¸ì´ê³ , í™•ì¥ì .bibëŠ” ì“°ì§€ ì•ŠëŠ”ë‹¤. ë§Œì•½ .bib íŒŒì¼ ì•ˆì— ë“¤ì–´ ìˆëŠ” ëª¨ë“  ë…¼ë¬¸ì„ í•œêº¼ë²ˆì— ì°¸ê³ ë¬¸í—Œìœ¼ë¡œ ì¶œë ¥í•˜ê³  ì‹¶ë‹¤ë©´ `\nocite{*}`ë¥¼ í•¨ê»˜ ë„£ì–´ ì£¼ë©´ ëœë‹¤.
                
                ì—¬ê¸°ì„œ `\bibliographystyle{unsrt}`ëŠ” ì°¸ê³ ë¬¸í—Œì˜ ì¶œë ¥ í˜•ì‹ê³¼ ì •ë ¬ ë°©ì‹ì„ ì§€ì •í•˜ëŠ” ì—­í• ì„ í•œë‹¤. unsrtëŠ” â€œì •ë ¬í•˜ì§€ ì•ŠëŠ”ë‹¤(unsorted)â€ëŠ” ëœ»ìœ¼ë¡œ, ë³¸ë¬¸ì—ì„œ ì¸ìš©ëœ ìˆœì„œ ê·¸ëŒ€ë¡œ ì°¸ê³ ë¬¸í—Œì„ ë‚˜ì—´í•˜ë¼ëŠ” ì˜ë¯¸ë‹¤. ì¦‰, ì„œë¡ ì—ì„œ ì²˜ìŒ ì¸ìš©í•œ ë…¼ë¬¸ì´ 1ë²ˆ, ê·¸ë‹¤ìŒì— ì¸ìš©í•œ ë…¼ë¬¸ì´ 2ë²ˆì´ ë˜ëŠ” ë°©ì‹ì´ë‹¤. ì´ ë°©ì‹ì€ ìì—°ê³¼í•™, ì˜ìƒëª… ë¶„ì•¼ ë…¼ë¬¸ì´ë‚˜ ìº¡ìŠ¤í†¤ ë³´ê³ ì„œì—ì„œ ê°€ì¥ í”íˆ ì“°ì´ë©°, ë…ìê°€ ë³¸ë¬¸ íë¦„ì„ ë”°ë¼ê°€ë©´ì„œ ì°¸ê³ ë¬¸í—Œì„ í™•ì¸í•˜ê¸° ì‰½ë‹¤ëŠ” ì¥ì ì´ ìˆë‹¤.
                
                ì´ë ‡ê²Œ .bib íŒŒì¼ì„ ì—…ë¡œë“œí•˜ê³ , ë¬¸ì„œ ë§¨ ì•„ë˜ì— `\bibliographystyle{unsrt}`ì™€ `\bibliography{bib íŒŒì¼ ì´ë¦„}`ë¥¼ ì¶”ê°€í•œ ë’¤ Recompile ë²„íŠ¼ì„ ëˆ„ë¥´ë©´, Overleafê°€ LaTeXì™€ BibTeXë¥¼ ìë™ìœ¼ë¡œ ì‹¤í–‰í•´ ì£¼ê³  PDFì— ì°¸ê³ ë¬¸í—Œ ëª©ë¡ì„ ë§Œë“¤ì–´ ì¤€ë‹¤. ì‚¬ìš©ìëŠ” ì»´íŒŒì¼ ìˆœì„œë¥¼ ì‹ ê²½ ì“¸ í•„ìš”ê°€ ì—†ê³ , íŒŒì¼ ì´ë¦„ë§Œ ì •í™•íˆ ë§ì¶”ë©´ ëœë‹¤.
                
                ì •ë¦¬í•˜ë©´, Overleafì—ì„œ BibTeXë¥¼ ì“°ëŠ” í•µì‹¬ì€ â€œ.bib íŒŒì¼ì€ ë°ì´í„°, .tex íŒŒì¼ì€ ì´ë¥¼ ì¶œë ¥í•˜ëŠ” ë„êµ¬â€ë¼ëŠ” ì ì„ ì´í•´í•˜ê³ , ë¬¸ì„œ ëì— ì°¸ê³ ë¬¸í—Œ ìŠ¤íƒ€ì¼ê³¼ ë°ì´í„° íŒŒì¼ì„ ì§€ì •í•´ ì£¼ëŠ” ê²ƒì´ë‹¤. `\bibliographystyle{unsrt}`ëŠ” ê·¸ì¤‘ì—ì„œë„ â€œì°¸ê³ ë¬¸í—Œì„ ì–´ë–¤ ê·œì¹™ìœ¼ë¡œ ë³´ì—¬ì¤„ì§€â€ë¥¼ ì •í•˜ëŠ” ì¤‘ìš”í•œ í•œ ì¤„ì´ë¼ê³  ë³´ë©´ ëœë‹¤.
                """)
                
                st.code(r"""
\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\title{hello world}
\author{Checkmate 137.}
\date{January 2026}
\begin{document}
\maketitle
\section{Introduction}
  
#ì•„ë˜ 3ì¤„ ë³µì‚¬ ë¶™ì—¬ë„£ê¸°
======================
\nocite{*}
\bibliographystyle{unsrt}
\bibliography{bibíŒŒì¼ ì´ë¦„}
======================
  
\end{document}
""", language="latex")
            
            st.divider()

            # [New] ì •ë ¬ ë°©ì‹ ì„ íƒ
            col_sort, _ = st.columns([2, 5])
            with col_sort:
                inv_sort_opt = st.selectbox("ì •ë ¬ ë°©ì‹", ["ì €ì¥í•œ ìˆœì„œ", "ê°€ì¹˜ ë†’ì€ ìˆœì„œ"])
            
            # ì›ë³¸ ë°ì´í„° ì°¸ì¡°
            inv_list = st.session_state.inventory
            
            # ì •ë ¬ ë¡œì§ (ì›ë³¸ ë³´ì¡´ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ ë³µì‚¬ë³¸ ì‚¬ìš©ì´ ì•„ë‹Œ, ê°ì²´ ì°¸ì¡° ì •ë ¬)
            if inv_sort_opt == "ê°€ì¹˜ ë†’ì€ ìˆœì„œ":
                # final_scoreê°€ ìˆìœ¼ë©´ ê·¸ê²ƒì„, ì—†ìœ¼ë©´ debiased_scoreë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬
                display_items = sorted(inv_list, key=lambda x: x.get('final_score', x.get('debiased_score', 0)), reverse=True)
            else:
                display_items = inv_list

            cols = st.columns(2)
            for i, paper in enumerate(display_items):
                # Key collision prevention using Paper ID
                p_id = paper['id']
                
                with cols[i % 2]:
                    with st.container(border=True):
                        status_emoji = "â“"; status_text = "ë¯¸ê²€ì¦"
                        if paper['is_reviewed']:
                            if paper['potential_type'] == "amazing": status_emoji, status_text = "âœ¨", "ëŒ€ì„±ê³µ"
                            elif paper['potential_type'] == "bad": status_emoji, status_text = "ğŸ’€", "ì‹¤íŒ¨"
                            elif paper['potential_type'] == "verified_user": status_emoji, status_text = "ğŸ›¡ï¸", "ì‚¬ìš©ì ìŠ¹ì¸"
                            else: status_emoji, status_text = "âœ…", "ê²€ì¦ë¨"

                        # [Changed] Title Display with Tooltip & Translation & Highlight
                        translated_title = get_translated_title(paper['title'])
                        display_title = highlight_text(paper['title']) if show_highlight else paper['title']
                        
                        st.markdown(
                            f"""<div title="[ë²ˆì—­] {translated_title}" style="font-size:1rem; font-weight:bold; margin-bottom:5px;">{display_title}</div>""", 
                            unsafe_allow_html=True
                        )
                        if show_translation:
                            st.caption(f"ğŸ‡°ğŸ‡· {translated_title}")
                        
                        st.caption(f"{status_emoji} {status_text} | {paper['journal']}")
                        
                        c_btn1, c_btn2 = st.columns([2, 1])
                        with c_btn1:
                            if not paper['is_reviewed']:
                                if paper['integrity_status'] == "valid":
                                    if st.button("ğŸ”¬ ì‹¬ì¸µ ê²€ì¦", key=f"rev_{p_id}", type="primary", use_container_width=True):
                                        # ê°ì²´ ì§ì ‘ ìˆ˜ì • (Reference Update)
                                        paper['is_reviewed'] = True
                                        bonus = int(paper['debiased_score'] * 0.5)
                                        st.session_state.score += bonus
                                        paper['final_score'] = paper['debiased_score'] + bonus
                                        if paper['potential_type'] == 'amazing': st.toast(f"ëŒ€ë°•! ìˆ¨ê²¨ì§„ ëª…ì‘ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤! (+{bonus})", icon="ğŸ‰")
                                        else: st.toast(f"ê²€ì¦ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. (+{bonus})", icon="âœ…")
                                        save_user_data(st.session_state.user_id) 
                                        st.rerun()
                                else:
                                    st.warning(paper['risk_reason'])
                                    if st.button("ê°•ì œ ìŠ¹ì¸", key=f"force_{p_id}", use_container_width=True):
                                        paper['is_reviewed'] = True
                                        bonus = 10 
                                        st.session_state.score += bonus
                                        paper['final_score'] = paper['debiased_score'] + bonus
                                        paper['potential_type'] = "verified_user"
                                        paper['reason'] = "ì‚¬ìš©ì ì§ì ‘ í™•ì¸ìœ¼ë¡œ ê²€ì¦ë¨"
                                        save_user_data(st.session_state.user_id) 
                                        st.rerun()
                            else:
                                st.success(f"ê°€ì¹˜: {paper.get('final_score', 0)}ì ")

                        with c_btn2:
                            if st.button("ì‚­ì œ", key=f"del_{p_id}", use_container_width=True):
                                deduction = paper.get('final_score', paper.get('debiased_score', 0))
                                st.session_state.score = max(0, st.session_state.score - deduction)
                                
                                # ID ê¸°ë°˜ ì‚­ì œ (ì •ë ¬ ìƒíƒœì™€ ë¬´ê´€í•˜ê²Œ ì•ˆì „í•˜ê²Œ ì‚­ì œ)
                                st.session_state.inventory = [p for p in st.session_state.inventory if p['id'] != p_id]
                                st.session_state.trash.append(paper)
                                
                                st.toast(f"ë…¼ë¬¸ ì‚­ì œ. {deduction}ì  ì°¨ê°ë¨", icon="ğŸ—‘ï¸")
                                save_user_data(st.session_state.user_id) 
                                st.rerun()
                        st.markdown(f"[ğŸ“„ ì›ë¬¸ ë³´ê¸°]({paper['url']})")

with tab_trash:
    if not st.session_state.trash: st.info("íœ´ì§€í†µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
    if st.session_state.trash:
        if st.button("íœ´ì§€í†µ ë¹„ìš°ê¸°", type="primary"):
            st.session_state.trash = []
            save_user_data(st.session_state.user_id)
            st.toast("íœ´ì§€í†µ ë¹„ì›€", icon="ğŸ§¹")
            st.rerun()
    cols = st.columns(2)
    for i, paper in enumerate(st.session_state.trash):
        with cols[i % 2]:
            with st.container(border=True):
                # [Changed] Title Display with Tooltip & Translation & Highlight
                translated_title = get_translated_title(paper['title'])
                display_title = highlight_text(paper['title']) if show_highlight else paper['title']
                
                st.markdown(
                    f"""<div title="[ë²ˆì—­] {translated_title}" style="font-size:1rem; font-weight:bold; color:gray; margin-bottom:5px;">{display_title}</div>""", 
                    unsafe_allow_html=True
                )
                if show_translation:
                    st.caption(f"ğŸ‡°ğŸ‡· {translated_title}")
                
                st.caption(f"ì‚­ì œë¨ | {paper['journal']}")
                c1, c2 = st.columns(2)
                with c1:
                    if st.button("ë³µêµ¬", key=f"rest_{i}", use_container_width=True):
                        restored = st.session_state.trash.pop(i)
                        st.session_state.inventory.append(restored)
                        r_score = restored.get('final_score', restored.get('debiased_score', 0))
                        st.session_state.score += r_score
                        st.toast(f"ë³µêµ¬ ì™„ë£Œ (+{r_score}ì )", icon="â™»ï¸")
                        save_user_data(st.session_state.user_id)
                        st.rerun()
                with c2:
                    if st.button("ì˜êµ¬ ì‚­ì œ", key=f"pdel_{i}", use_container_width=True):
                        st.session_state.trash.pop(i)
                        st.toast("ì˜êµ¬ ì‚­ì œë¨", icon="ğŸ”¥")
                        save_user_data(st.session_state.user_id)
                        st.rerun()
